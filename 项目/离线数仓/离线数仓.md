# 离线数仓

## 项目描述

**数据来源**

- 业务系统数据库数据
- 前端埋点日志
- 爬虫数据

**使用工具**

Flume：可分布式日志收集系统，框架比较清晰，比如source，channel，sink的概念。而且是用java开发的，二次开发也很方便。大公司都在用，表现稳定，方便实时监控传输数据

Sqoop：基于MR程序，且只有Map任务



## 项目需求

1. 用户行为数据采集
2. 业务数据采集
3. 数据仓库维度建模
4. 分析指标（设备、会员、商品、地区、活动等电商主题）100多个
5. 即席查询支持临时任务
6. 集群性能监控
7. 元数据管理
8. 数据质量监控
9. 权限管理



## 技术选型

- 数据采集：`Flume`、`Kafka`、`Sqoop`、`Logstash`、`DataX`
- 数据存储：`MySQL`、`HDFS`、`HBase`、`Redis`、`MongoDB`
- 数据计算：`Hive`、`Tez`、`Spark`、`Flink`、`Storm`
- 数据查询：`Presto`、`Kylin`、`Impala`、`Druid`、`ClickHouse`、`Doris`
- 数据可视化：`Echarts`、`Superset`、`QuickBI`、`DataV`
- 任务调度：`Azkaban`、`Ooize`、`DolpinSceduler`、`AirFlow`
- 集群监控：`Zabbix`、`Prometheus`
- 元数据管理：`Atlas`
- 权限管理：`Ranger`、`Sentry`



**选型**

ELK应用于数据量不是巨大的场合，DataX和Sqoop目前市场占有率五五开

MySQL用于数据量小的场合，一般存结果数据（ADS）

HBase一般存Kylin预计算数据，Rdis可用于实时计算中

Tez数据完全放在内存，Spark数据部分在内存，部分在磁盘

在离线中这次使用Zabbix，在实时中使用Prometheus

元数据管理Atlas解决问题：查看任务影响范围（指标输出）

Ranger控制哪些用户可以看哪些表，Sentry已经被Apache除名了



**系统流程**

![img](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/01_%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E9%87%87%E9%9B%86wps44A3.tmp.png)



**框架版本**

采用Apache版本，因为CDH版本开始收费，不收费版本太过老旧

![image-20220212221844272](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/01_%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E9%87%87%E9%9B%86image-20220212221844272.png)



## 采集内容

**页面信息**

页面id：本页面id

上页id：从哪个页面过来的

页面对象类型：商品

页面对象id：商品id

页面来源类型：搜索、跳转....

停留时间：30000秒

跳入时间：时间戳类型



**事件信息**

添加收藏、取消收藏、添加购物车.......



**曝光信息**

曝光类型：商品推广、算法推荐商品、查询结果商品、促销活动

曝光对象类型：商品skuId、活动id

曝光对象id

曝光顺序



**启动信息**

启动入口：图标、通知、安装后启动

启动加载时间

开屏广告id

广告播放时间

用户跳过广告时间

启动时间



**错误信息**

错误码、错误信息



## 集群配置

**压缩类型优缺点**

目前在Hadoop中用得比较多的有lzo，gzip，snappy，bzip2这4种压缩格式，实践中应根据实际情况选择不同的压缩格式。

**1、gzip压缩**

优点：压缩率比较高，而且压缩/解压速度也比较快；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便。

缺点：不支持split。

应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用gzip压缩格式。譬如说一天或者一个小时的日志压缩成一个gzip文件，运行mapreduce程序的时候通过多个gzip文件达到并发。hive程序，streaming程序，和java写的mapreduce程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。

**2、lzo压缩**

优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；可以在linux系统下安装lzop命令，使用方便。

缺点：压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）。

应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越越明显。

**3、snappy压缩**

优点：高速压缩速度和合理的压缩率；支持hadoop native库。

缺点：不支持split；压缩率比gzip要低；hadoop本身不支持，需要安装；linux系统下没有对应的命令。

应用场景：当mapreduce作业的map输出的数据比较大的时候，作为map到reduce的中间数据的压缩格式；或者作为一个mapreduce作业的输出和另外一个mapreduce作业的输入。

**4、bzip2压缩**

优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便。

缺点：压缩/解压速度慢；不支持native。

应用场景：适合对速度要求不高，但需要较高的压缩率的时候，可以作为mapreduce作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。





Orc文件描述：[Hive数仓建表该选用ORC还是Parquet，压缩选LZO还是Snappy？ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/257917645)

**lzo压缩**

LZO相比较压缩，更追求速度，如果想让数据可以切分，需要创建LZO索引

不创建索引的后果是，不管文件有多大，都会只有一个Map读取





## Hadoop配置

**前期准备**

集群分发脚本

```shell
#!/bin/bash
#1. 判断参数个数
if [ $# -lt 1 ]
then
  echo Not Enough Arguement!
  exit;
fi
#2. 遍历集群所有机器
for host in hadoop102 hadoop103 hadoop104
do
  echo ====================  $host  ====================
  #3. 遍历所有目录，挨个发送
  for file in $@
  do
    #4 判断文件是否存在
    if [ -e $file ]
    then
      #5. 获取父目录
      pdir=$(cd -P $(dirname $file); pwd)
      #6. 获取当前文件的名称
      fname=$(basename $file)
      ssh $host "mkdir -p $pdir"
      rsync -av $pdir/$fname $host:$pdir
    else
      echo $file does not exists!
    fi
  done
done
```

集群启停脚本

```shell
#!/bin/bash
if [ $# -lt 1 ]
then
    echo "No Args Input..."
    exit ;
fi
case $1 in
"start")
        echo " =================== 启动 hadoop集群 ==================="

        echo " --------------- 启动 hdfs ---------------"
        ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"
        echo " --------------- 启动 yarn ---------------"
        ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
        echo " --------------- 启动 historyserver ---------------"
        ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver"
;;
"stop")
        echo " =================== 关闭 hadoop集群 ==================="

        echo " --------------- 关闭 historyserver ---------------"
        ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"
        echo " --------------- 关闭 yarn ---------------"
        ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
        echo " --------------- 关闭 hdfs ---------------"
        ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"
;;
*)
    echo "Input Args Error..."
;;
esac
```



### **1. Hadoop HDFS多目录**

HDFS的DataNode节点保存数据的路径由dfs.datanode.data.dir参数决定，其默认值为`file://${hadoop.tmp.dir}/dfs/data`，若服务器有多个磁盘，必须对该参数进行修改。如服务器磁盘有多个，则该参数应修改为如下的值。

其中值为磁盘名

```xml
<property>
    <name>dfs.datanode.data.dir</name>
  <value>file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4</value>
</property>
```



### **2. 集群数据均衡**

**节点间数据均衡**

1. 开启数据均衡命令

```shell
start-balancer.sh -threshold 10
```

对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整。

2. 停止数据均衡命令

```shell
stop-balancer.sh
```

如果不停止，Hadoop将会一直数据均衡下去

注意：于HDFS需要启动单独的Rebalance Server来执行Rebalance操作，[所以尽量不要在NameNode上执行start-balancer.sh](http://xn--NameNodestart-balancer-sy68a2bx05ag98dux2afg9ay6af924fm2hzl3e.sh)，而是找一台比较空闲的机器。

**磁盘间数据均衡**

1）生成均衡计划（**只有一块磁盘的话，不会生成计划**）

```
hdfs diskbalancer -plan hadoop103
```

2）执行均衡计划（**执行第一步操作产生的Json文件**）

```
hdfs diskbalancer -execute hadoop103.plan.json
```

3）查看当前均衡任务的执行情况

```
hdfs diskbalancer -query hadoop103
```

4）取消均衡任务（**如果不停止，将会一直数据均衡下去**）

```
hdfs diskbalancer -cancel hadoop103.plan.json
```



### **3. LZO压缩配置**

hadoop本身并不支持lzo压缩，故需要使用twitter提供的hadoop-lzo开源组件。hadoop-lzo需依赖hadoop和lzo进行编译，编译步骤可上网搜索

编译后的`hadoop-lzo-0.4.20.jar`放入`hadoop-3.1.3/share/hadoop/common/`

**core-site.xml配置支持LZO压缩**

```xml
<configuration>
    <property>
        <name>io.compression.codecs</name>
        <value>
            org.apache.hadoop.io.compress.GzipCodec,
            org.apache.hadoop.io.compress.DefaultCodec,
            org.apache.hadoop.io.compress.BZip2Codec,
            org.apache.hadoop.io.compress.SnappyCodec,
            com.hadoop.compression.lzo.LzoCodec,
            com.hadoop.compression.lzo.LzopCodec
        </value>
    </property>

    <property>
        <name>io.compression.codec.lzo.class</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>
</configuration>
```

**LZO创建索引**

LZO压缩文件的可切片特性依赖于其索引，故我们需要手动为LZO压缩文件创建索引。若无索引，则LZO文件的切片只有一个。

```shell
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar  com.hadoop.compression.lzo.DistributedLzoIndexer lzo文件
```

之后执行wordcount，发现split文件数变成多个

```shell
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class=com.hadoop.mapreduce.LzoTextInputFormat /input /output2
```



**参数调优**

HDFS *hdfs-site.xml*调优

```xml
The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.
NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。
对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。

<property>
    <name>dfs.namenode.handler.count</name>
    <value>10</value>
</property>
```



Yarn *yarn-site.xml*调优

NodeManager内存和服务器实际内存配置尽量接近，如服务器有128g内存，但是NodeManager默认内存8G，不修改该参数最多只能用8G内存。NodeManager使用的CPU核数和服务器CPU核数尽量接近。

例如 8G -> 100G

CPU核数提升到15核

```
yarn.nodemanager.resource.memory-mb	    NodeManager使用内存数
yarn.nodemanager.resource.cpu-vcores	NodeManager使用CPU核数
```





## Zookeeper安装

启停脚本

```shell
#!/bin/bash
case $1 in
"start"){
	for i in hadoop102 hadoop103 hadoop104
	do
        echo ---------- zookeeper $i 启动 ------------
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh start"
			done
};;
"stop"){
	for i in hadoop102 hadoop103 hadoop104
	do
        echo ---------- zookeeper $i 停止 ------------    
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop"
	done
};;
"status"){
	for i in hadoop102 hadoop103 hadoop104
	do
        echo ---------- zookeeper $i 状态 ------------    
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh status"
	done
};;
esac
```

增加脚本执行权限

```shell
chmod u+x zk.sh
```

Zookeeper集群启动脚本

```shell
zk.sh start
```

Zookeeper集群停止脚本

```shell
zk.sh stop
```



## Kafka安装

集群启动停止脚本

```shell
#! /bin/bash
case $1 in
"start"){
    for i in hadoop102 hadoop103 hadoop104
    do
        echo " --------启动 $i Kafka-------"
        ssh $i "/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties"
    done
};;
"stop"){
    for i in hadoop102 hadoop103 hadoop104
    do
        echo " --------停止 $i Kafka-------"
        ssh $i "/opt/module/kafka/bin/kafka-server-stop.sh stop"
    done
};;
esac
```

启停

```shell
chmod u+x kf.sh
kf.sh start
kf.sh stop
```



**Kafka分区数计算**

（1）创建一个只有1个分区的topic

（2）测试这个topic的producer吞吐量和consumer吞吐量。

（3）假设他们的值分别是Tp和Tc，单位可以是MB/s。

（4）然后假设总的目标吞吐量是Tt，那么分区数 = Tt / min（Tp，Tc）

例如：producer吞吐量 = 20m/s；consumer吞吐量 = 50m/s，期望吞吐量100m/s；

分区数 = 100 / 20 = 5分区

https://blog.csdn.net/weixin_42641909/article/details/89294698

分区数一般设置为：3-10个

## Flume

**Source**

（1）Taildir Source相比Exec Source、Spooling Directory Source的优势

TailDir Source：断点续传、多目录。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传。不会丢数据，但是有可能会导致数据重复。

Exec Source可以实时搜集数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失。

Spooling Directory Source监控目录，支持断点续传。

（2）batchSize大小如何设置？

答：Event 1K左右时，500-1000合适（默认为100）

**Channel**

采用Kafka Channel，省去了Sink，提高了效率。KafkaChannel数据存储在Kafka里面，所以数据是存储在磁盘中。

注意在**Flume1.7**以前，Kafka Channel很少有人使用，因为发现**parseAsFlumeEvent**这个配置起不了作用。也就是无论**parseAsFlumeEvent**配置为true还是false，都会转为Flume Event。这样的话，造成的结果是，会始终都把Flume的headers中的信息混合着内容一起写入Kafka的消息中，这显然不是我所需要的，我只是需要把内容写入即可。

**parseAsFlumeEvent**：写入Kafka时，是否带上Flume头信息，一般选false



**编写配置**

```shell
#为各组件命名
a1.sources = r1
a1.channels = c1

#描述source
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.*
a1.sources.r1.positionFile = /opt/module/flume/taildir_position.json
a1.sources.r1.interceptors =  i1
a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.ETLInterceptor$Builder

#描述channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
#写一个kafka broker也行，因为通过一个broker就可以连接到kafka集群
a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092
a1.channels.c1.kafka.topic = topic_log
#丢弃flume的消息头
a1.channels.c1.parseAsFlumeEvent = false

#绑定source和channel以及sink和channel的关系
a1.sources.r1.channels = c1
```



**启停脚本**

```shell
#! /bin/bash

case $1 in
"start"){
        for i in hadoop102 hadoop103
        do
                echo " --------启动 $i 采集flume-------"
                ssh $i "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume/log1.txt 2>&1  &"
        done
};;	
"stop"){
        for i in hadoop102 hadoop103
        do
                echo " --------停止 $i 采集flume-------"
                ssh $i "ps -ef | grep file-flume-kafka | grep -v grep |awk  '{print \$2}' | xargs -n1 kill -9 "
        done

};;
esac
```

说明1：nohup，该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。nohup就是不挂起的意思，不挂断地运行命令。

说明2：awk 默认分隔符为空格

说明3：$2是在“”双引号内部会被解析为脚本的第二个参数，但是这里面想表达的含义是awk的第二个值，所以需要将他转义，用\$2表示。

说明4：xargs 表示取出前面命令运行的结果，作为后面命令的输入参数。

```
chmod u+x f1.sh
f1.sh start
f1.sh stop
```



**FileChannel底层原理**

存储：存储在磁盘中

索引：存储在内存中

备份索引：存储在磁盘中，也可以对备份索引进行一个再备份

磁盘配置了多目录的话，可以提升并发度（需要是多块磁盘）

### 零点漂移问题

**零点漂移**：一个数据23:59分产生，00:05分到达，被写到下一天的分区里面

原《阿里巴巴大数据之路》给出方案：多读取下一天15分钟内容，取出数据时间反写到上一天分区里面

现在可以用Flume时间拦截器解决这个问题

拦截器读出数据中的时间内容，作为这个数据的时间写到正确的分区里面

解决的思路：拦截json日志，通过fastjson框架解析json，获取实际时间ts。将获取的ts时间写入拦截器header头，header的key必须是timestamp，因为Flume框架会根据这个key的值识别为时间，写入到HDFS。



**配置示例**

```shell
## source1
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
# 多少数据量一批
a1.sources.r1.batchSize = 5000
# 或多久时间后算一批
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r1.kafka.topics=topic_log
a1.sources.r1.interceptors = i1
# 时间拦截器
a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.TimeStampInterceptor$Builder

## channel1
a1.channels.c1.type = file
# 索引备份位置
a1.channels.c1.checkpointDir = /opt/module/flume/checkpoint/behavior1
# 数据存储位置
a1.channels.c1.dataDirs = /opt/module/flume/data/behavior1/


## sink1
a1.sinks.k1.type = hdfs
# %Y-%m-%d：使用头信息中的时间作为文件夹名字
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log-
# 是否开启按某种时间单位滚动形成文件，例如分钟、小时，和下面的roll不一样
a1.sinks.k1.hdfs.round = false

#控制生成的小文件
# 控制指定秒后形成一个文件
a1.sinks.k1.hdfs.rollInterval = 10
# 控制指定128M形成一个文件
a1.sinks.k1.hdfs.rollSize = 134217728
# 控制指定多少条后形成一个文件
a1.sinks.k1.hdfs.rollCount = 0

## 控制输出文件是原生文件。
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = lzop

## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1
```



**时间拦截器**

```java
import com.alibaba.fastjson.JSONObject;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.StandardCharsets;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

public class TimeStampInterceptor implements Interceptor {

    private ArrayList<Event> events = new ArrayList<>();

    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {

        Map<String, String> headers = event.getHeaders();
        String log = new String(event.getBody(), StandardCharsets.UTF_8);

        JSONObject jsonObject = JSONObject.parseObject(log);

        String ts = jsonObject.getString("ts");
        headers.put("timestamp", ts);

        return event;
    }

    @Override
    public List<Event> intercept(List<Event> list) {
        events.clear();
        for (Event event : list) {
            events.add(intercept(event));
        }

        return events;
    }

    @Override
    public void close() {

    }

    public static class Builder implements Interceptor.Builder {
        @Override
        public Interceptor build() {
            return new TimeStampInterceptor();
        }

        @Override
        public void configure(Context context) {
        }
    }
}
```





### Flume优化

如果消费Flume抛出以下异常

```
ERROR hdfs.HDFSEventSink: process failed
java.lang.OutOfMemoryError: GC overhead limit exceeded
```

在hadoop102服务器的/opt/module/flume/conf/flume-env.sh文件中增加如下配置

```
export JAVA_OPTS="-Xms100m -Xmx2000m -Dcom.sun.management.jmxremote
```

JVM heap一般设置为4G或更高

-Xmx与-Xms最好设置一致，减少内存抖动带来的性能影响，如果设置不一致容易导致频繁fullgc。

-Xms表示JVM Heap（堆内存）最小尺寸，初始分配；-Xmx 表示JVM Heap(堆内存)最大允许的尺寸，按需分配。如果不设置一致，容易在初始化时，由于内存不够，频繁触发fullgc。



## 采集通道启停脚本

```shell
#!/bin/bash

case $1 in
"start"){
        echo ================== 启动 集群 ==================

        #启动 Zookeeper集群
        zk.sh start

        #启动 Hadoop集群
        hdp.sh start

        #启动 Kafka采集集群
        kf.sh start

        #启动 Flume采集集群
        f1.sh start

        #启动 Flume消费集群
        f2.sh start
        
        };;
"stop"){
        echo ================== 停止 集群 ==================

        #停止 Flume消费集群
        f2.sh stop

        #停止 Flume采集集群
        f1.sh stop

        #停止 Kafka采集集群
        kf.sh stop

        #停止 Hadoop集群
        hdp.sh stop

        #停止 Zookeeper集群
        zk.sh stop

};;
esac
```



## 2NN显示不出页面问题

访问2NN页面[http://hadoop104:9868](http://hadoop104:9868/)，看不到详细信息

（1）在浏览器上按F12，查看问题原因。定位bug在61行

（2）找到要修改的文件

```
[atguigu@hadoop102 static]$ pwd
/opt/module/hadoop-3.1.3/share/hadoop/hdfs/webapps/static

[atguigu@hadoop102 static]$ vim dfs-dust.js
:set nu

修改61行 
return new Date(Number(v)).toLocaleString();
```

（3）分发dfs-dust.js

```
[atguigu@hadoop102 static]$ xsync dfs-dust.js
```



## sqoop

```shell
bin/sqoop import \
--connect jdbc:mysql://hadoop102:3306/gmall \
--username root \
--password 000000 \
--table user_info \
--columns id,login_name \
--where "id>=10 and id<=30" \    # 原始表where条件
# 4 5 6 行参数也可以用下面的参数替代  $CONDITIONS：sqoop占位符，运行时会被动态替换
--query ‘select id,login_name from user_info where id>=10 and id<=30 and $CONDITIONS’
--target-dir /test \             # 目标hdfs文件夹
--delete-target-dir \            # 导入前如文件夹存在，删除掉
--fields-terminated-by '\t' \    # hdfs分隔符
--num-mappers 2 \                # 切成几个map任务
--split-by id                    # map任务按那个字段来切
```

### 同步策略

**全量同步策略**：每天存储一个全量数据，作为一个分区，适合表数据量不大情况，且每天有新数据插入以及旧数据修改情况

**增量同步策略**：每天存储一份增量数据，适用于表数据量大，且每天只有新数据插入情况

**新增及变化策略**：使用表数据量大，且既有新增又有变化场景，处理方式较复杂

某些特殊的表，可不必遵循上述同步策略。例如某些不会发生变化的表（地区表，省份表，民族表）可以只存一份固定值。



### 导出策略

由于导出只支持HDFS到关系型数据库，想要增量导出的话，需要先指定关系型数据库的主键，并设置好更新模式

**更新模式**

1. insert模式：所有数据都当作新数据插入
2. updateonly：只更新数据，新增数据不插入
3. allowinsert：既更新数据，又插入新数据

```shell
#!/bin/bash

hive_db_name=gmall
mysql_db_name=gmall_report

export_data() {
/opt/module/sqoop/bin/sqoop export \
--connect "jdbc:mysql://hadoop102:3306/${mysql_db_name}?useUnicode=true&characterEncoding=utf-8"  \
--username root \
--password 000000 \
--table $1 \
--num-mappers 1 \
--export-dir /warehouse/$hive_db_name/ads/$1 \
--input-fields-terminated-by "\t" \
--update-mode allowinsert \  # 更新模式
--update-key $2 \            # 指定主键
--input-null-string '\\N'    \
--input-null-non-string '\\N'
```





**首日同步脚本**

Hive中的Null在底层是以“\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。在导出数据时采用--input-null-string和--input-null-non-string两个参数。导入数据时采用--null-string和--null-non-string。

```shell
#! /bin/bash

APP=gmall
sqoop=/opt/module/sqoop/bin/sqoop

if [ -n "$2" ] ;then
   do_date=$2
else 
   echo "请传入日期参数"
   exit
fi 

import_data(){
$sqoop import \
--connect jdbc:mysql://hadoop102:3306/$APP \
--username root \
--password 000000 \
--target-dir /origin_data/$APP/db/$1/$do_date \
--delete-target-dir \
--query "$2 where \$CONDITIONS" \
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \
--compression-codec lzop \
--null-string '\\N' \
--null-non-string '\\N'

hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date
}

import_order_info(){
  import_data order_info "select
                            id, 
                            total_amount, 
                            order_status, 
                            user_id, 
                            payment_way,
                            delivery_address,
                            out_trade_no, 
                            create_time, 
                            operate_time,
                            expire_time,
                            tracking_no,
                            province_id,
                            activity_reduce_amount,
                            coupon_reduce_amount,                            
                            original_total_amount,
                            feight_fee,
                            feight_fee_reduce      
                        from order_info"
}

import_coupon_use(){
  import_data coupon_use "select
                          id,
                          coupon_id,
                          user_id,
                          order_id,
                          coupon_status,
                          get_time,
                          using_time,
                          used_time,
                          expire_time
                        from coupon_use"
}

import_order_status_log(){
  import_data order_status_log "select
                                  id,
                                  order_id,
                                  order_status,
                                  operate_time
                                from order_status_log"
}

import_user_info(){
  import_data "user_info" "select 
                            id,
                            login_name,
                            nick_name,
                            name,
                            phone_num,
                            email,
                            user_level, 
                            birthday,
                            gender,
                            create_time,
                            operate_time
                          from user_info"
}

import_order_detail(){
  import_data order_detail "select 
                              id,
                              order_id, 
                              sku_id,
                              sku_name,
                              order_price,
                              sku_num, 
                              create_time,
                              source_type,
                              source_id,
                              split_total_amount,
                              split_activity_amount,
                              split_coupon_amount
                            from order_detail"
}

import_payment_info(){
  import_data "payment_info"  "select 
                                id,  
                                out_trade_no, 
                                order_id, 
                                user_id, 
                                payment_type, 
                                trade_no, 
                                total_amount,  
                                subject, 
                                payment_status,
                                create_time,
                                callback_time 
                              from payment_info"
}

import_comment_info(){
  import_data comment_info "select
                              id,
                              user_id,
                              sku_id,
                              spu_id,
                              order_id,
                              appraise,
                              create_time
                            from comment_info"
}

import_order_refund_info(){
  import_data order_refund_info "select
                                id,
                                user_id,
                                order_id,
                                sku_id,
                                refund_type,
                                refund_num,
                                refund_amount,
                                refund_reason_type,
                                refund_status,
                                create_time
                              from order_refund_info"
}

import_sku_info(){
  import_data sku_info "select 
                          id,
                          spu_id,
                          price,
                          sku_name,
                          sku_desc,
                          weight,
                          tm_id,
                          category3_id,
                          is_sale,
                          create_time
                        from sku_info"
}

import_base_category1(){
  import_data "base_category1" "select 
                                  id,
                                  name 
                                from base_category1"
}

import_base_category2(){
  import_data "base_category2" "select
                                  id,
                                  name,
                                  category1_id 
                                from base_category2"
}

import_base_category3(){
  import_data "base_category3" "select
                                  id,
                                  name,
                                  category2_id
                                from base_category3"
}

import_base_province(){
  import_data base_province "select
                              id,
                              name,
                              region_id,
                              area_code,
                              iso_code,
                              iso_3166_2
                            from base_province"
}

import_base_region(){
  import_data base_region "select
                              id,
                              region_name
                            from base_region"
}

import_base_trademark(){
  import_data base_trademark "select
                                id,
                                tm_name
                              from base_trademark"
}

import_spu_info(){
  import_data spu_info "select
                            id,
                            spu_name,
                            category3_id,
                            tm_id
                          from spu_info"
}

import_favor_info(){
  import_data favor_info "select
                          id,
                          user_id,
                          sku_id,
                          spu_id,
                          is_cancel,
                          create_time,
                          cancel_time
                        from favor_info"
}

import_cart_info(){
  import_data cart_info "select
                        id,
                        user_id,
                        sku_id,
                        cart_price,
                        sku_num,
                        sku_name,
                        create_time,
                        operate_time,
                        is_ordered,
                        order_time,
                        source_type,
                        source_id
                      from cart_info"
}

import_coupon_info(){
  import_data coupon_info "select
                          id,
                          coupon_name,
                          coupon_type,
                          condition_amount,
                          condition_num,
                          activity_id,
                          benefit_amount,
                          benefit_discount,
                          create_time,
                          range_type,
                          limit_num,
                          taken_count,
                          start_time,
                          end_time,
                          operate_time,
                          expire_time
                        from coupon_info"
}

import_activity_info(){
  import_data activity_info "select
                              id,
                              activity_name,
                              activity_type,
                              start_time,
                              end_time,
                              create_time
                            from activity_info"
}

import_activity_rule(){
    import_data activity_rule "select
                                    id,
                                    activity_id,
                                    activity_type,
                                    condition_amount,
                                    condition_num,
                                    benefit_amount,
                                    benefit_discount,
                                    benefit_level
                                from activity_rule"
}

import_base_dic(){
    import_data base_dic "select
                            dic_code,
                            dic_name,
                            parent_code,
                            create_time,
                            operate_time
                          from base_dic"
}


import_order_detail_activity(){
    import_data order_detail_activity "select
                                           id,
                                           order_id,
                                           order_detail_id,
                                           activity_id,
                                           activity_rule_id,
                                           sku_id,
                                           create_time
                                       from order_detail_activity"
}


import_order_detail_coupon(){
    import_data order_detail_coupon "select
                                        id,
								        order_id,
                                        order_detail_id,
                                        coupon_id,
                                        coupon_use_id,
                                        sku_id,
                                        create_time
                                    from order_detail_coupon"
}


import_refund_payment(){
    import_data refund_payment "select
                                    id,
                                    out_trade_no,
                                    order_id,
                                    sku_id,
                                    payment_type,
                                    trade_no,
                                    total_amount,
                                    subject,
                                    refund_status,
                                    create_time,
                                    callback_time
                                from refund_payment"                                                    

}

import_sku_attr_value(){
    import_data sku_attr_value "select
                                    id,
                                    attr_id,
                                    value_id,
                                    sku_id,
                                    attr_name,
                                    value_name
                                from sku_attr_value"
}


import_sku_sale_attr_value(){
    import_data sku_sale_attr_value "select
                                        id,
                                        sku_id,
                                        spu_id,
                                        sale_attr_value_id,
                                        sale_attr_id,
                                        sale_attr_name,
                                        sale_attr_value_name
                                    from sku_sale_attr_value"
}

case $1 in
  "order_info")
     import_order_info
;;
  "base_category1")
     import_base_category1
;;
  "base_category2")
     import_base_category2
;;
  "base_category3")
     import_base_category3
;;
  "order_detail")
     import_order_detail
;;
  "sku_info")
     import_sku_info
;;
  "user_info")
     import_user_info
;;
  "payment_info")
     import_payment_info
;;
  "base_province")
     import_base_province
;;
  "base_region")
     import_base_region
;;
  "base_trademark")
     import_base_trademark
;;
  "activity_info")
      import_activity_info
;;
  "cart_info")
      import_cart_info
;;
  "comment_info")
      import_comment_info
;;
  "coupon_info")
      import_coupon_info
;;
  "coupon_use")
      import_coupon_use
;;
  "favor_info")
      import_favor_info
;;
  "order_refund_info")
      import_order_refund_info
;;
  "order_status_log")
      import_order_status_log
;;
  "spu_info")
      import_spu_info
;;
  "activity_rule")
      import_activity_rule
;;
  "base_dic")
      import_base_dic
;;
  "order_detail_activity")
      import_order_detail_activity
;;
  "order_detail_coupon")
      import_order_detail_coupon
;;
  "refund_payment")
      import_refund_payment
;;
  "sku_attr_value")
      import_sku_attr_value
;;
  "sku_sale_attr_value")
      import_sku_sale_attr_value
;;
  "all")
   import_base_category1
   import_base_category2
   import_base_category3
   import_order_info
   import_order_detail
   import_sku_info
   import_user_info
   import_payment_info
   import_base_region
   import_base_province
   import_base_trademark
   import_activity_info
   import_cart_info
   import_comment_info
   import_coupon_use
   import_coupon_info
   import_favor_info
   import_order_refund_info
   import_order_status_log
   import_spu_info
   import_activity_rule
   import_base_dic
   import_order_detail_activity
   import_order_detail_coupon
   import_refund_payment
   import_sku_attr_value
   import_sku_sale_attr_value
;;
esac
```

说明1：

[ -n 变量值 ] 判断变量的值，是否为空

-- 变量的值，非空，返回true

-- 变量的值，为空，返回false

说明2：

查看date命令的使用，[atguigu@hadoop102 ~]$ date --help



**增量同步**

```shell
#! /bin/bash

APP=gmall
sqoop=/opt/module/sqoop/bin/sqoop

if [ -n "$2" ] ;then
    do_date=$2
else
    do_date=`date -d '-1 day' +%F`
fi

import_data(){
$sqoop import \
--connect jdbc:mysql://hadoop102:3306/$APP \
--username root \
--password 000000 \
--target-dir /origin_data/$APP/db/$1/$do_date \
--delete-target-dir \
--query "$2 and  \$CONDITIONS" \
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \
--compression-codec lzop \
--null-string '\\N' \
--null-non-string '\\N'

hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date
}

import_order_info(){
  import_data order_info "select
                            id, 
                            total_amount, 
                            order_status, 
                            user_id, 
                            payment_way,
                            delivery_address,
                            out_trade_no, 
                            create_time, 
                            operate_time,
                            expire_time,
                            tracking_no,
                            province_id,
                            activity_reduce_amount,
                            coupon_reduce_amount,                            
                            original_total_amount,
                            feight_fee,
                            feight_fee_reduce      
                        from order_info
                        where (date_format(create_time,'%Y-%m-%d')='$do_date' 
                        or date_format(operate_time,'%Y-%m-%d')='$do_date')"
}

import_coupon_use(){
  import_data coupon_use "select
                          id,
                          coupon_id,
                          user_id,
                          order_id,
                          coupon_status,
                          get_time,
                          using_time,
                          used_time,
                          expire_time
                        from coupon_use
                        where (date_format(get_time,'%Y-%m-%d')='$do_date'
                        or date_format(using_time,'%Y-%m-%d')='$do_date'
                        or date_format(used_time,'%Y-%m-%d')='$do_date'
                        or date_format(expire_time,'%Y-%m-%d')='$do_date')"
}

import_order_status_log(){
  import_data order_status_log "select
                                  id,
                                  order_id,
                                  order_status,
                                  operate_time
                                from order_status_log
                                where date_format(operate_time,'%Y-%m-%d')='$do_date'"
}

import_user_info(){
  import_data "user_info" "select 
                            id,
                            login_name,
                            nick_name,
                            name,
                            phone_num,
                            email,
                            user_level, 
                            birthday,
                            gender,
                            create_time,
                            operate_time
                          from user_info 
                          where (DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date' 
                          or DATE_FORMAT(operate_time,'%Y-%m-%d')='$do_date')"
}

import_order_detail(){
  import_data order_detail "select 
                              id,
                              order_id, 
                              sku_id,
                              sku_name,
                              order_price,
                              sku_num, 
                              create_time,
                              source_type,
                              source_id,
                              split_total_amount,
                              split_activity_amount,
                              split_coupon_amount
                            from order_detail 
                            where DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date'"
}

import_payment_info(){
  import_data "payment_info"  "select 
                                id,  
                                out_trade_no, 
                                order_id, 
                                user_id, 
                                payment_type, 
                                trade_no, 
                                total_amount,  
                                subject, 
                                payment_status,
                                create_time,
                                callback_time 
                              from payment_info 
                              where (DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date' 
                              or DATE_FORMAT(callback_time,'%Y-%m-%d')='$do_date')"
}

import_comment_info(){
  import_data comment_info "select
                              id,
                              user_id,
                              sku_id,
                              spu_id,
                              order_id,
                              appraise,
                              create_time
                            from comment_info
                            where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

import_order_refund_info(){
  import_data order_refund_info "select
                                id,
                                user_id,
                                order_id,
                                sku_id,
                                refund_type,
                                refund_num,
                                refund_amount,
                                refund_reason_type,
                                refund_status,
                                create_time
                              from order_refund_info
                              where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

import_sku_info(){
  import_data sku_info "select 
                          id,
                          spu_id,
                          price,
                          sku_name,
                          sku_desc,
                          weight,
                          tm_id,
                          category3_id,
                          is_sale,
                          create_time
                        from sku_info where 1=1"
}

import_base_category1(){
  import_data "base_category1" "select 
                                  id,
                                  name 
                                from base_category1 where 1=1"
}

import_base_category2(){
  import_data "base_category2" "select
                                  id,
                                  name,
                                  category1_id 
                                from base_category2 where 1=1"
}

import_base_category3(){
  import_data "base_category3" "select
                                  id,
                                  name,
                                  category2_id
                                from base_category3 where 1=1"
}

import_base_province(){
  import_data base_province "select
                              id,
                              name,
                              region_id,
                              area_code,
                              iso_code,
                              iso_3166_2
                            from base_province
                            where 1=1"
}

import_base_region(){
  import_data base_region "select
                              id,
                              region_name
                            from base_region
                            where 1=1"
}

import_base_trademark(){
  import_data base_trademark "select
                                id,
                                tm_name
                              from base_trademark
                              where 1=1"
}

import_spu_info(){
  import_data spu_info "select
                            id,
                            spu_name,
                            category3_id,
                            tm_id
                          from spu_info
                          where 1=1"
}

import_favor_info(){
  import_data favor_info "select
                          id,
                          user_id,
                          sku_id,
                          spu_id,
                          is_cancel,
                          create_time,
                          cancel_time
                        from favor_info
                        where 1=1"
}

import_cart_info(){
  import_data cart_info "select
                        id,
                        user_id,
                        sku_id,
                        cart_price,
                        sku_num,
                        sku_name,
                        create_time,
                        operate_time,
                        is_ordered,
                        order_time,
                        source_type,
                        source_id
                      from cart_info
                      where 1=1"
}

import_coupon_info(){
  import_data coupon_info "select
                          id,
                          coupon_name,
                          coupon_type,
                          condition_amount,
                          condition_num,
                          activity_id,
                          benefit_amount,
                          benefit_discount,
                          create_time,
                          range_type,
                          limit_num,
                          taken_count,
                          start_time,
                          end_time,
                          operate_time,
                          expire_time
                        from coupon_info
                        where 1=1"
}

import_activity_info(){
  import_data activity_info "select
                              id,
                              activity_name,
                              activity_type,
                              start_time,
                              end_time,
                              create_time
                            from activity_info
                            where 1=1"
}

import_activity_rule(){
    import_data activity_rule "select
                                    id,
                                    activity_id,
                                    activity_type,
                                    condition_amount,
                                    condition_num,
                                    benefit_amount,
                                    benefit_discount,
                                    benefit_level
                                from activity_rule
                                where 1=1"
}

import_base_dic(){
    import_data base_dic "select
                            dic_code,
                            dic_name,
                            parent_code,
                            create_time,
                            operate_time
                          from base_dic
                          where 1=1"
}


import_order_detail_activity(){
    import_data order_detail_activity "select
                                                                id,
                                                                order_id,
                                                                order_detail_id,
                                                                activity_id,
                                                                activity_rule_id,
                                                                sku_id,
                                                                create_time
                                                            from order_detail_activity
                                                            where date_format(create_time,'%Y-%m-%d')='$do_date'"
}


import_order_detail_coupon(){
    import_data order_detail_coupon "select
                                                                id,
								                                                order_id,
                                                                order_detail_id,
                                                                coupon_id,
                                                                coupon_use_id,
                                                                sku_id,
                                                                create_time
                                                            from order_detail_coupon
                                                            where date_format(create_time,'%Y-%m-%d')='$do_date'"
}


import_refund_payment(){
    import_data refund_payment "select
                                                        id,
                                                        out_trade_no,
                                                        order_id,
                                                        sku_id,
                                                        payment_type,
                                                        trade_no,
                                                        total_amount,
                                                        subject,
                                                        refund_status,
                                                        create_time,
                                                        callback_time
                                                    from refund_payment
                                                    where (DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date' 
                                                    or DATE_FORMAT(callback_time,'%Y-%m-%d')='$do_date')"                                                    

}

import_sku_attr_value(){
    import_data sku_attr_value "select
                                                    id,
                                                    attr_id,
                                                    value_id,
                                                    sku_id,
                                                    attr_name,
                                                    value_name
                                                from sku_attr_value
                                                where 1=1"
}


import_sku_sale_attr_value(){
    import_data sku_sale_attr_value "select
                                                            id,
                                                            sku_id,
                                                            spu_id,
                                                            sale_attr_value_id,
                                                            sale_attr_id,
                                                            sale_attr_name,
                                                            sale_attr_value_name
                                                        from sku_sale_attr_value
                                                        where 1=1"
}

case $1 in
  "order_info")
     import_order_info
;;
  "base_category1")
     import_base_category1
;;
  "base_category2")
     import_base_category2
;;
  "base_category3")
     import_base_category3
;;
  "order_detail")
     import_order_detail
;;
  "sku_info")
     import_sku_info
;;
  "user_info")
     import_user_info
;;
  "payment_info")
     import_payment_info
;;
  "base_province")
     import_base_province
;;
  "activity_info")
      import_activity_info
;;
  "cart_info")
      import_cart_info
;;
  "comment_info")
      import_comment_info
;;
  "coupon_info")
      import_coupon_info
;;
  "coupon_use")
      import_coupon_use
;;
  "favor_info")
      import_favor_info
;;
  "order_refund_info")
      import_order_refund_info
;;
  "order_status_log")
      import_order_status_log
;;
  "spu_info")
      import_spu_info
;;
  "activity_rule")
      import_activity_rule
;;
  "base_dic")
      import_base_dic
;;
  "order_detail_activity")
      import_order_detail_activity
;;
  "order_detail_coupon")
      import_order_detail_coupon
;;
  "refund_payment")
      import_refund_payment
;;
  "sku_attr_value")
      import_sku_attr_value
;;
  "sku_sale_attr_value")
      import_sku_sale_attr_value
;;
"all")
   import_base_category1
   import_base_category2
   import_base_category3
   import_order_info
   import_order_detail
   import_sku_info
   import_user_info
   import_payment_info
   import_base_trademark
   import_activity_info
   import_cart_info
   import_comment_info
   import_coupon_use
   import_coupon_info
   import_favor_info
   import_order_refund_info
   import_order_status_log
   import_spu_info
   import_activity_rule
   import_base_dic
   import_order_detail_activity
   import_order_detail_coupon
   import_refund_payment
   import_sku_attr_value
   import_sku_sale_attr_value
;;
esac
```



## Hive

**Hive on Spark配置**

官网下载的Hive3.1.2和Spark3.0.0默认是不兼容的。因为Hive3.1.2支持的Spark版本是2.4.5，所以需要我们重新编译Hive3.1.2版本。

编译步骤：官网下载Hive3.1.2源码，修改pom文件中引用的Spark版本为3.0.0，如果编译通过，直接打包获取jar包。如果报错，就根据提示，修改相关方法，直到不报错，打包获取jar包。



**向HDFS上传Spark纯净版jar包**

​	说明1：由于Spark3.0.0非纯净版默认支持的是hive2.3.7版本，直接使用会和安装的Hive3.1.2出现兼容性问题。所以采用Spark纯净版jar包，不包含hadoop和hive相关依赖，避免冲突。

​	说明2：Hive任务最终由Spark来执行，Spark任务资源分配由Yarn来调度，该任务有可能被分配到集群的任何一个节点。所以需要将Spark的依赖上传到HDFS集群路径，这样集群中任何一个节点都能获取到。



## Yarn配置

**增加Application Master资源比例**

容量调度器对每个资源队列中同时运行的Application Master占用的资源进行了限制，该限制通过yarn.scheduler.capacity.maximum-am-resource-percent参数实现，其默认值是0.1，表示每个资源队列上Application Master最多可使用的资源为该队列总资源的10%，目的是防止大部分资源都被Application Master占用，而导致Map/Reduce Task无法执行。

生产环境该参数可使用默认值。但学习环境，集群资源总数很少，如果只分配10%的资源给Application Master，则可能出现，同一时刻只能运行一个Job的情况，因为一个Application Master使用的资源就可能已经达到10%的上限了。故此处可将该值适当调大



## Lzo索引失效

hive.input.format，其默认值为CombineHiveInputFormat，其会先将索引文件当成小文件合并，将其当做普通文件处理。更严重的是，这会导致LZO文件无法切片。

**解决办法**：修改CombineHiveInputFormat为HiveInputFormat

```
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat
```



## 拉链表

获取数据全量切片： 生效开始日期 <= 指定日期 and 生效结束日期 >= 指定日期

```sql
-- 获取2022-04-24切片
where start_date <= '2022-04-24' and end_date >= '2022-04-24'
```

**拉链表分区**：按照end_date字段分区

思路：取每日更新数据，和分区为`9999-99-99`数据关联

```sql
with
tmp as
(
    select
        old.id old_id,
        old.login_name old_login_name,
        old.nick_name old_nick_name,
        old.name old_name,
        old.phone_num old_phone_num,
        old.email old_email,
        old.user_level old_user_level,
        old.birthday old_birthday,
        old.gender old_gender,
        old.create_time old_create_time,
        old.operate_time old_operate_time,
        old.start_date old_start_date,
        old.end_date old_end_date,
        new.id new_id,
        new.login_name new_login_name,
        new.nick_name new_nick_name,
        new.name new_name,
        new.phone_num new_phone_num,
        new.email new_email,
        new.user_level new_user_level,
        new.birthday new_birthday,
        new.gender new_gender,
        new.create_time new_create_time,
        new.operate_time new_operate_time,
        new.start_date new_start_date,
        new.end_date new_end_date
    from
    (
        select
            id,
            login_name,
            nick_name,
            name,
            phone_num,
            email,
            user_level,
            birthday,
            gender,
            create_time,
            operate_time,
            start_date,
            end_date
        from dim_user_info
        where dt='9999-99-99'
    )old
    full outer join
    (
        select
            id,
            login_name,
            nick_name,
            md5(name) name,
            md5(phone_num) phone_num,
            md5(email) email,
            user_level,
            birthday,
            gender,
            create_time,
            operate_time,
            '2020-06-15' start_date,
            '9999-99-99' end_date
        from ods_user_info
        where dt='2020-06-15'
    )new
    on old.id=new.id
)
insert overwrite table dim_user_info partition(dt)
select
    nvl(new_id,old_id),
    nvl(new_login_name,old_login_name),
    nvl(new_nick_name,old_nick_name),
    nvl(new_name,old_name),
    nvl(new_phone_num,old_phone_num),
    nvl(new_email,old_email),
    nvl(new_user_level,old_user_level),
    nvl(new_birthday,old_birthday),
    nvl(new_gender,old_gender),
    nvl(new_create_time,old_create_time),
    nvl(new_operate_time,old_operate_time),
    nvl(new_start_date,old_start_date),
    nvl(new_end_date,old_end_date),
    nvl(new_end_date,old_end_date) dt
from tmp
union all
select
    old_id,
    old_login_name,
    old_nick_name,
    old_name,
    old_phone_num,
    old_email,
    old_user_level,
    old_birthday,
    old_gender,
    old_create_time,
    old_operate_time,
    old_start_date,
    cast(date_add('2020-06-15',-1) as string),
    cast(date_add('2020-06-15',-1) as string) dt
from tmp
where new_id is not null and old_id is not null;
```



## UDTF编写

摘取官方文档一句话

```
A custom UDTF can be created by extending the GenericUDTF abstract class and then implementing the initialize, process, and possibly close methods. The initialize method is called by Hive to notify the UDTF the argument types to expect. The UDTF must then return an object inspector corresponding to the row objects that the UDTF will generate. Once initialize() has been called, Hive will give rows to the UDTF using the process() method. While in process(), the UDTF can produce and forward rows to other operators by calling forward(). Lastly, Hive will call the close() method when all the rows have passed to the UDTF.
```

Hive SQL会被转换成一连串的Operator Tree，Operator 类似Spark的算子

object inspector：对象检查器，数据在两个Operator中间传递时候，数据和类型是分离的，数据存在Java Object中，类型存在object inspector中；object inspector还提供了用于解析Object对象中所保存数据的方法；

**翻译**：

```shell
一个自定义的UDTF需要继承抽象类GenericUDTF并实现它的方法（initialize, process, 可选的close方法）
initialize方法会被Hive调用去通知UDTF函数将要接收的参数类型
		# Hive调用initialize方法，将上一个Operator返回的object inspector对象检查器传给UDTF
UDTF必须返回一个object inspector对象检查器--与UDTF输出对应的对象检查器
		# 用于通知下一个Operator对象检查器的类型
一旦initialize方法被调用，Hive将一行行数据交给process方法调用
process方法编写自定义逻辑，产生多行数据，通过forward方法输出给其他的Operator
当所有数据处理完成后，Hive调用close方法
```

代码示例：

将JSON数组转成多行JSON字符串

```java
package com.atguigu.hive.udtf;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
import org.json.JSONArray;

import java.util.ArrayList;
import java.util.List;

public class ExplodeJSONArray extends GenericUDTF {

    @Override
    public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {

        // 1 参数合法性检查
        if (argOIs.length != 1) {
            throw new UDFArgumentException("explode_json_array 只需要一个参数");
        }

        // 2 第一个参数必须为string
        //判断参数是否为基础数据类型
        if (argOIs[0].getCategory() != ObjectInspector.Category.PRIMITIVE) {
            throw new UDFArgumentException("explode_json_array 只接受基础类型参数");
        }

        //将参数对象检查器强转为基础类型对象检查器
        PrimitiveObjectInspector argumentOI = (PrimitiveObjectInspector) argOIs[0];

        //判断参数是否为String类型
        if (argumentOI.getPrimitiveCategory() != PrimitiveObjectInspector.PrimitiveCategory.STRING) {
            throw new UDFArgumentException("explode_json_array 只接受string类型的参数");
        }

        // 3 定义返回值名称和类型
        List<String> fieldNames = new ArrayList<String>();
        List<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
		
        // 返回列名
        fieldNames.add("items");
        // 返回类型
        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);

        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
    }

    public void process(Object[] objects) throws HiveException {

        // 1 获取传入的数据
        String jsonArray = objects[0].toString();

        // 2 将string转换为json数组
        JSONArray actions = new JSONArray(jsonArray);

        // 3 循环一次，取出数组中的一个json，并写出
        for (int i = 0; i < actions.length(); i++) {

            String[] result = new String[1];
            result[0] = actions.getString(i);
            // 返回数据，根据initialize方法内容，返回数组形式数据
            forward(result);
        }
    }

    public void close() throws HiveException {

    }

}
```



**创建永久函数**

1. jar包上传HDFS
2. 创建永久函数与开发好的java class关联

```
create function explode_json_array as 'com.hive.udtf.ExplodeJSONArray' using jar '.......';
```

注意：如果修改了自定义函数重新生成jar包怎么处理？只需要替换HDFS路径上的旧jar包，然后重启Hive客户端即可。

**创建临时函数**

```java
add jar /path/xx.jar（存储在本地磁盘）

// 临时注册UDF函数（hive会话生效）
create temporary function 函数名 as '包名.类名';

//删除临时函数：
drop temporary function 数据库名.函数名;
```



## 累计快照型事实表

将已完成不再更新的数据放在历史分区中，未完成数据放在`9999-99-99`分区中

dwd表中`9999-99-99`分区和ods表中新增及变化的数据full join，实现动态分区

join后得到的已完成数据放在历史分区中

```sql
insert overwrite table dwd_payment_info partition(dt)
select
    nvl(new.id,old.id),
    nvl(new.order_id,old.order_id),
    nvl(new.user_id,old.user_id),
    nvl(new.province_id,old.province_id),
    nvl(new.trade_no,old.trade_no),
    nvl(new.out_trade_no,old.out_trade_no),
    nvl(new.payment_type,old.payment_type),
    nvl(new.payment_amount,old.payment_amount),
    nvl(new.payment_status,old.payment_status),
    nvl(new.create_time,old.create_time),
    nvl(new.callback_time,old.callback_time),
    nvl(date_format(nvl(new.callback_time,old.callback_time),'yyyy-MM-dd'),'9999-99-99')
from
(
    select id,
       order_id,
       user_id,
       province_id,
       trade_no,
       out_trade_no,
       payment_type,
       payment_amount,
       payment_status,
       create_time,
       callback_time
    from dwd_payment_info
    where dt = '9999-99-99'
)old
full outer join
(
    select
        pi.id,
        pi.out_trade_no,
        pi.order_id,
        pi.user_id,
        oi.province_id,
        pi.payment_type,
        pi.trade_no,
        pi.payment_amount,
        pi.payment_status,
        pi.create_time,
        pi.callback_time
    from
    (
        select * from ods_payment_info where dt='2020-06-15'
    )pi
    left join
    (
        select id,province_id from ods_order_info where dt='2020-06-15'
    )oi
    on pi.order_id=oi.id
)new
on old.id=new.id;
```



**极端情况**

某天所有数据都是已完成数据，9999-99-99分区应该是空的，但由于没法覆盖，导致有值

解决办法：查看9999-99-99分区的文件修改时间，不是当天的话，就清空

```shell
clear_data(){
    current_date=`date +%F`
    current_date_timestamp=`date -d "$current_date" +%s`

    last_modified_date=`hadoop fs -ls /warehouse/gmall/dwd/$1 | grep '9999-99-99' | awk '{print $6}'`
    last_modified_date_timestamp=`date -d "$last_modified_date" +%s`

    if [[ $last_modified_date_timestamp -lt $current_date_timestamp ]]; then
        echo "clear table $1 partition(dt=9999-99-99)"
        hadoop fs -rm -r -f /warehouse/gmall/dwd/$1/dt=9999-99-99/*
    fi
}

# 调用函数
clear_data dwd_order_info
```



## 常用函数

```
collect_set( named_struct() )
str_to_map(concat_ws(',',collect_set(concat( col1 ,'=', col2))),',','=')
date_format('2020-06-14','yyyy-MM')
date_add('2020-06-14',-1)
next_day('2020-06-14','MO')  # 下周一
last_day('2020-06-14')       # 某月最后一天
array_contains(array[], '')  # 判断数组是否包含这字段
```



## ODS层

日装载脚本

```shell
#!/bin/bash

# 定义变量方便修改
APP=gmall

# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   do_date=$1
else 
   do_date=`date -d "-1 day" +%F`
fi 

echo ================== 日志日期为 $do_date ==================
sql="
load data inpath '/origin_data/$APP/log/topic_log/$do_date' into table ${APP}.ods_log partition(dt='$do_date');
"

hive -e "$sql"

hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/$APP/ods/ods_log/dt=$do_date
```



## DWD层

**事务型事实表**

针对一些一经产生就无法变更的数据，将每日增量数据传到日分区里面

![image-20220504181420723](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/01_%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E9%87%87%E9%9B%86image-20220504181420723.png)



**周期型事实表**

采用每日全量

用于针对数据量比较小或者每天变动较大的数据

![image-20220504181657121](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220504181657121.png)



**累计快照型事实表**

[ctrl + 鼠标左键跳转](#累计快照型事实表)



## DWS层

1. 根据维度统计指标，得到结果作为dws表
2. 一对多关系的话，且要聚合也要看明细的话，可以使用array[struct()]结构，例如 用户 -> 访问页面明细

**一个新的聚合思路，使用空列补0方式实现性能更好的聚合**

针对数据来自不同表，插入同一个dws表场景

```sql
with
tmp_order as
(
    select
        date_format(create_time,'yyyy-MM-dd') dt,
        sku_id,
        count(*) order_count,
        sum(sku_num) order_num,
        sum(if(split_activity_amount>0,1,0)) order_activity_count,
        sum(if(split_coupon_amount>0,1,0)) order_coupon_count,
        sum(split_activity_amount) order_activity_reduce_amount,
        sum(split_coupon_amount) order_coupon_reduce_amount,
        sum(original_amount) order_original_amount,
        sum(split_final_amount) order_final_amount
    from dwd_order_detail
    group by date_format(create_time,'yyyy-MM-dd'),sku_id
),
tmp_pay as
(
    select
        date_format(callback_time,'yyyy-MM-dd') dt,
        sku_id,
        count(*) payment_count,
        sum(sku_num) payment_num,
        sum(split_final_amount) payment_amount
    from dwd_order_detail od
    join
    (
        select
            order_id,
            callback_time
        from dwd_payment_info
        where callback_time is not null
    )pi on pi.order_id=od.order_id
    group by date_format(callback_time,'yyyy-MM-dd'),sku_id
),
tmp_ri as
(
    select
        date_format(create_time,'yyyy-MM-dd') dt,
        sku_id,
        count(*) refund_order_count,
        sum(refund_num) refund_order_num,
        sum(refund_amount) refund_order_amount
    from dwd_order_refund_info
    group by date_format(create_time,'yyyy-MM-dd'),sku_id
),
tmp_rp as
(
    select
        date_format(callback_time,'yyyy-MM-dd') dt,
        rp.sku_id,
        count(*) refund_payment_count,
        sum(ri.refund_num) refund_payment_num,
        sum(refund_amount) refund_payment_amount
    from
    (
        select
            order_id,
            sku_id,
            refund_amount,
            callback_time
        from dwd_refund_payment
    )rp
    left join
    (
        select
            order_id,
            sku_id,
            refund_num
        from dwd_order_refund_info
    )ri
    on rp.order_id=ri.order_id
    and rp.sku_id=ri.sku_id
    group by date_format(callback_time,'yyyy-MM-dd'),rp.sku_id
),
tmp_cf as
(
    select
        dt,
        item sku_id,
        sum(if(action_id='cart_add',1,0)) cart_count,
        sum(if(action_id='favor_add',1,0)) favor_count
    from dwd_action_log
    where action_id in ('cart_add','favor_add')
    group by dt,item
),
tmp_comment as
(
    select
        date_format(create_time,'yyyy-MM-dd') dt,
        sku_id,
        sum(if(appraise='1201',1,0)) appraise_good_count,
        sum(if(appraise='1202',1,0)) appraise_mid_count,
        sum(if(appraise='1203',1,0)) appraise_bad_count,
        sum(if(appraise='1204',1,0)) appraise_default_count
    from dwd_comment_info
    group by date_format(create_time,'yyyy-MM-dd'),sku_id
)
insert overwrite table dws_sku_action_daycount partition(dt)
select
    sku_id,
    sum(order_count),
    sum(order_num),
    sum(order_activity_count),
    sum(order_coupon_count),
    sum(order_activity_reduce_amount),
    sum(order_coupon_reduce_amount),
    sum(order_original_amount),
    sum(order_final_amount),
    sum(payment_count),
    sum(payment_num),
    sum(payment_amount),
    sum(refund_order_count),
    sum(refund_order_num),
    sum(refund_order_amount),
    sum(refund_payment_count),
    sum(refund_payment_num),
    sum(refund_payment_amount),
    sum(cart_count),
    sum(favor_count),
    sum(appraise_good_count),
    sum(appraise_mid_count),
    sum(appraise_bad_count),
    sum(appraise_default_count),
    dt
from
(
    select
        dt,
        sku_id,
        order_count,
        order_num,
        order_activity_count,
        order_coupon_count,
        order_activity_reduce_amount,
        order_coupon_reduce_amount,
        order_original_amount,
        order_final_amount,
        0 payment_count,
        0 payment_num,
        0 payment_amount,
        0 refund_order_count,
        0 refund_order_num,
        0 refund_order_amount,
        0 refund_payment_count,
        0 refund_payment_num,
        0 refund_payment_amount,
        0 cart_count,
        0 favor_count,
        0 appraise_good_count,
        0 appraise_mid_count,
        0 appraise_bad_count,
        0 appraise_default_count
    from tmp_order
    union all
    select
        dt,
        sku_id,
        0 order_count,
        0 order_num,
        0 order_activity_count,
        0 order_coupon_count,
        0 order_activity_reduce_amount,
        0 order_coupon_reduce_amount,
        0 order_original_amount,
        0 order_final_amount,
        payment_count,
        payment_num,
        payment_amount,
        0 refund_order_count,
        0 refund_order_num,
        0 refund_order_amount,
        0 refund_payment_count,
        0 refund_payment_num,
        0 refund_payment_amount,
        0 cart_count,
        0 favor_count,
        0 appraise_good_count,
        0 appraise_mid_count,
        0 appraise_bad_count,
        0 appraise_default_count
    from tmp_pay
    union all
    select
        dt,
        sku_id,
        0 order_count,
        0 order_num,
        0 order_activity_count,
        0 order_coupon_count,
        0 order_activity_reduce_amount,
        0 order_coupon_reduce_amount,
        0 order_original_amount,
        0 order_final_amount,
        0 payment_count,
        0 payment_num,
        0 payment_amount,
        refund_order_count,
        refund_order_num,
        refund_order_amount,
        0 refund_payment_count,
        0 refund_payment_num,
        0 refund_payment_amount,
        0 cart_count,
        0 favor_count,
        0 appraise_good_count,
        0 appraise_mid_count,
        0 appraise_bad_count,
        0 appraise_default_count
    from tmp_ri
    union all
    select
        dt,
        sku_id,
        0 order_count,
        0 order_num,
        0 order_activity_count,
        0 order_coupon_count,
        0 order_activity_reduce_amount,
        0 order_coupon_reduce_amount,
        0 order_original_amount,
        0 order_final_amount,
        0 payment_count,
        0 payment_num,
        0 payment_amount,
        0 refund_order_count,
        0 refund_order_num,
        0 refund_order_amount,
        refund_payment_count,
        refund_payment_num,
        refund_payment_amount,
        0 cart_count,
        0 favor_count,
        0 appraise_good_count,
        0 appraise_mid_count,
        0 appraise_bad_count,
        0 appraise_default_count
    from tmp_rp
    union all
    select
        dt,
        sku_id,
        0 order_count,
        0 order_num,
        0 order_activity_count,
        0 order_coupon_count,
        0 order_activity_reduce_amount,
        0 order_coupon_reduce_amount,
        0 order_original_amount,
        0 order_final_amount,
        0 payment_count,
        0 payment_num,
        0 payment_amount,
        0 refund_order_count,
        0 refund_order_num,
        0 refund_order_amount,
        0 refund_payment_count,
        0 refund_payment_num,
        0 refund_payment_amount,
        cart_count,
        favor_count,
        0 appraise_good_count,
        0 appraise_mid_count,
        0 appraise_bad_count,
        0 appraise_default_count
    from tmp_cf
    union all
    select
        dt,
        sku_id,
        0 order_count,
        0 order_num,
        0 order_activity_count,
        0 order_coupon_count,
        0 order_activity_reduce_amount,
        0 order_coupon_reduce_amount,
        0 order_original_amount,
        0 order_final_amount,
        0 payment_count,
        0 payment_num,
        0 payment_amount,
        0 refund_order_count,
        0 refund_order_num,
        0 refund_order_amount,
        0 refund_payment_count,
        0 refund_payment_num,
        0 refund_payment_amount,
        0 cart_count,
        0 favor_count,
        appraise_good_count,
        appraise_mid_count,
        appraise_bad_count,
        appraise_default_count
    from tmp_comment
)t1
group by dt,sku_id;
```



## DWT层

DWT层也是以维度作为主键，将各种DWS层表数据聚合到DWT层

与DWS层不同的是，DWT层会聚合一些累计值，例如**最近7天、最近30天**指标，甚至是历史所有的聚合值

对这些指标，最好不要直接计算最近7天、最近30天分区；而是把昨天分区数据拿出来，减去7天前、30天前分区数据，在加上今天分区的数据，就得到了新的累计值；

![image-20220504183607756](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220504183607756.png)

计算过程：

![image-20220504183703869](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220504183703869.png)



例子

```sql
DROP TABLE IF EXISTS dwt_visitor_topic;
CREATE EXTERNAL TABLE dwt_visitor_topic
(
    `mid_id` STRING COMMENT '设备id',
    `brand` STRING COMMENT '手机品牌',
    `model` STRING COMMENT '手机型号',
    `channel` ARRAY<STRING> COMMENT '渠道',
    `os` ARRAY<STRING> COMMENT '操作系统',
    `area_code` ARRAY<STRING> COMMENT '地区ID',
    `version_code` ARRAY<STRING> COMMENT '应用版本',
    `visit_date_first` STRING  COMMENT '首次访问时间',
    `visit_date_last` STRING  COMMENT '末次访问时间',
    `visit_last_1d_count` BIGINT COMMENT '最近1日访问次数',
    `visit_last_1d_day_count` BIGINT COMMENT '最近1日访问天数',
    `visit_last_7d_count` BIGINT COMMENT '最近7日访问次数',
    `visit_last_7d_day_count` BIGINT COMMENT '最近7日访问天数',
    `visit_last_30d_count` BIGINT COMMENT '最近30日访问次数',
    `visit_last_30d_day_count` BIGINT COMMENT '最近30日访问天数',
    `visit_count` BIGINT COMMENT '累积访问次数',
    `visit_day_count` BIGINT COMMENT '累积访问天数'
) COMMENT '设备主题宽表'
PARTITIONED BY (`dt` STRING)
STORED AS PARQUET
LOCATION '/warehouse/gmall/dwt/dwt_visitor_topic'
TBLPROPERTIES ("parquet.compression"="lzo");
```



```sql
insert overwrite table dwt_visitor_topic partition(dt='2020-06-14')
select
    nvl(1d_ago.mid_id,old.mid_id),
    nvl(1d_ago.brand,old.brand),
    nvl(1d_ago.model,old.model),
    nvl(1d_ago.channel,old.channel),
    nvl(1d_ago.os,old.os),
    nvl(1d_ago.area_code,old.area_code),
    nvl(1d_ago.version_code,old.version_code),
    case when old.mid_id is null and 1d_ago.is_new=1 then '2020-06-14'
         when old.mid_id is null and 1d_ago.is_new=0 then '2020-06-13'--无法获取准确的首次登录日期，给定一个数仓搭建日之前的日期
         else old.visit_date_first end,
    if(1d_ago.mid_id is not null,'2020-06-14',old.visit_date_last),
    nvl(1d_ago.visit_count,0),
    if(1d_ago.mid_id is null,0,1),
    nvl(old.visit_last_7d_count,0)+nvl(1d_ago.visit_count,0)- nvl(7d_ago.visit_count,0),
    nvl(old.visit_last_7d_day_count,0)+if(1d_ago.mid_id is null,0,1)- if(7d_ago.mid_id is null,0,1),
    nvl(old.visit_last_30d_count,0)+nvl(1d_ago.visit_count,0)- nvl(30d_ago.visit_count,0),
    nvl(old.visit_last_30d_day_count,0)+if(1d_ago.mid_id is null,0,1)- if(30d_ago.mid_id is null,0,1),
    nvl(old.visit_count,0)+nvl(1d_ago.visit_count,0),
    nvl(old.visit_day_count,0)+if(1d_ago.mid_id is null,0,1)
from
(
    select
        mid_id,
        brand,
        model,
        channel,
        os,
        area_code,
        version_code,
        visit_date_first,
        visit_date_last,
        visit_last_1d_count,
        visit_last_1d_day_count,
        visit_last_7d_count,
        visit_last_7d_day_count,
        visit_last_30d_count,
        visit_last_30d_day_count,
        visit_count,
        visit_day_count
    from dwt_visitor_topic
    where dt=date_add('2020-06-14',-1)
)old
full outer join
(
    select
        mid_id,
        brand,
        model,
        is_new,
        channel,
        os,
        area_code,
        version_code,
        visit_count
    from dws_visitor_action_daycount
    where dt='2020-06-14'
)1d_ago
on old.mid_id=1d_ago.mid_id
left join
(
    select
        mid_id,
        brand,
        model,
        is_new,
        channel,
        os,
        area_code,
        version_code,
        visit_count
    from dws_visitor_action_daycount
    where dt=date_add('2020-06-14',-7)
)7d_ago
on old.mid_id=7d_ago.mid_id
left join
(
    select
        mid_id,
        brand,
        model,
        is_new,
        channel,
        os,
        area_code,
        version_code,
        visit_count
    from dws_visitor_action_daycount
    where dt=date_add('2020-06-14',-30)
)30d_ago
on old.mid_id=30d_ago.mid_id;
```





## ADS层

### 累计值计算

**分别计算累计1天、7天、30天的指标方法**

**要求：**一套SQL计算完，不使用union，防止改逻辑时候要改三遍

**方法：**使用`explode`函数。将数据源扩大成三倍，分区数量不同，用`explode`函数产生的字段分组

```sql
with t1 as(
    select
        mid_id,
        channel,
        last_page_id,
        page_id,
        during_time,
        ts,
        recent_days -- 累计值分类
    from dwd_page_log 
        lateral view explode(Array(1,7,30)) tmp as recent_days
    where dt>=date_add('2020-06-14',-recent_days+1)
)
select
	...
from 
	t1
group by 
	mid_id,channel,recent_days
```

完整SQL

```sql
insert overwrite table ads_visit_stats
select * from ads_visit_stats
union
select
    '2020-06-14' dt,
    is_new,
    recent_days,
    channel,
    count(distinct(mid_id)) uv_count,
    cast(sum(duration)/1000 as bigint) duration_sec,
    cast(avg(duration)/1000 as bigint) avg_duration_sec,
    sum(page_count) page_count,
    cast(avg(page_count) as bigint) avg_page_count,
    count(*) sv_count,
    sum(if(page_count=1,1,0)) bounce_count,
    cast(sum(if(page_count=1,1,0))/count(*)*100 as decimal(16,2)) bounce_rate
from
(
    select
        session_id,
        mid_id,
        is_new,
        recent_days,
        channel,
        count(*) page_count,
        sum(during_time) duration
    from
    (
        select
            mid_id,
            channel,
            recent_days,
            is_new,
            last_page_id,
            page_id,
            during_time,
            concat(mid_id,'-',last_value(if(last_page_id is null,ts,null),true) over (partition by recent_days,mid_id order by ts)) session_id
        from
        (
            select
                mid_id,
                channel,
                last_page_id,
                page_id,
                during_time,
                ts,
                recent_days,
                if(visit_date_first>=date_add('2020-06-14',-recent_days+1),'1','0') is_new
            from
            (
                select
                    t1.mid_id,
                    t1.channel,
                    t1.last_page_id,
                    t1.page_id,
                    t1.during_time,
                    t1.dt,
                    t1.ts,
                    t2.visit_date_first
                from
                (
                    select
                        mid_id,
                        channel,
                        last_page_id,
                        page_id,
                        during_time,
                        dt,
                        ts
                    from dwd_page_log
                    where dt>=date_add('2020-06-14',-30)
                )t1
                left join
                (
                    select
                        mid_id,
                        visit_date_first
                    from dwt_visitor_topic
                    where dt='2020-06-14'
                )t2
                on t1.mid_id=t2.mid_id
            )t3 lateral view explode(Array(1,7,30)) tmp as recent_days
            where dt>=date_add('2020-06-14',-recent_days+1)
        )t4
    )t5
    group by session_id,mid_id,is_new,recent_days,channel
)t6
group by is_new,recent_days,channel;
```



### 路径分析

展示页面访问路径，一般使用**桑基图**

桑基图需要我们提供每种页面跳转的次数，每个跳转由source/target表示，source指跳转起始页面，target表示跳转终到页面。

```sql
DROP TABLE IF EXISTS ads_page_path;
CREATE EXTERNAL TABLE ads_page_path
(
    `dt` STRING COMMENT '统计日期',
    `recent_days` BIGINT COMMENT '最近天数,1:最近1天,7:最近7天,30:最近30天',
    `source` STRING COMMENT '跳转起始页面ID',
    `target` STRING COMMENT '跳转终到页面ID',
    `path_count` BIGINT COMMENT '跳转次数'
)  COMMENT '页面浏览路径'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LOCATION '/warehouse/gmall/ads/ads_page_path/';
```

1. **桑基图的source不允许为空，但target可为空**

2. **桑基图所展示的流程不允许存在环**



**解决问题1**

抛弃dwd表中last_page_id这个列，使用本行page_id作为source，下一行page_id作为target

```sql
page_id   source,
lead(page_id,1,null) over (partition by recent_days,session_id order by ts)  target
```



**解决问题2**

对每个session分组，按时间戳排序，source和target列拼接上需要

注意：target列拼接的序号要加一

```sql
concat('step-',rk,':',source)   source,
concat('step-',rk+1,':',target) target
```



完整SQL

```sql
insert overwrite table ads_page_path
select * from ads_page_path
union
select
    '2020-06-14',
    recent_days,
    source,
    target,
    count(*)
from
(
    select
        recent_days,
        concat('step-',step,':',source) source,
        concat('step-',step+1,':',target) target
    from
    (
        select
            recent_days,
            page_id source,
            lead(page_id,1,null) over (partition by recent_days,session_id order by ts) target,
            row_number() over (partition by recent_days,session_id order by ts) step
        from
        (
            select
                recent_days,
                last_page_id,
                page_id,
                ts,
                concat(mid_id,'-',last_value(if(last_page_id is null,ts,null),true) over (partition by mid_id,recent_days order by ts)) session_id
            from dwd_page_log lateral view explode(Array(1,7,30)) tmp as recent_days
            where dt>=date_add('2020-06-14',-30)
            and dt>=date_add('2020-06-14',-recent_days+1)
        )t2
    )t3
)t4
group by recent_days,source,target;
```



### **用户累计值计算**

1. 使用`explode`函数将数据分组
2. 根据`recent_days`的值选择不同的列，并分组后累计求值 

```sql
insert overwrite table ads_user_total
select * from ads_user_total
union
select
    '2020-06-14',
    recent_days,
    sum(if(login_date_first>=recent_days_ago,1,0))  new_user_count,
    sum(if(order_date_first>=recent_days_ago,1,0))  new_order_user_count,
    sum(order_final_amount)                         order_final_amount,
    sum(if(order_final_amount>0,1,0))               order_user_count,
    sum(if(login_date_last>=recent_days_ago and order_final_amount=0,1,0)) no_order_user_count
from
(
    select
        recent_days,
        user_id,
        login_date_first,
        login_date_last,
        order_date_first,
        case when recent_days=0  then  order_final_amount
             when recent_days=1  then  order_last_1d_final_amount
             when recent_days=7  then  order_last_7d_final_amount
             when recent_days=30 then  order_last_30d_final_amount
        end order_final_amount,
        if(recent_days=0,'1970-01-01',date_add('2020-06-14',-recent_days+1)) recent_days_ago
    from dwt_user_topic lateral view explode(Array(0,1,7,30)) tmp as recent_days
    where dt='2020-06-14'
)t1
group by recent_days;
```



### 用户变动

| 指标       | 说明                                                         | 对应字段             |
| ---------- | ------------------------------------------------------------ | -------------------- |
| 流失用户数 | 之前活跃过的用户，最近一段时间未活跃，就称为流失用户。此处要求统计7日前（只包含7日前当天）活跃，但最近7日未活跃的用户总数。 | user_churn_count     |
| 回流用户数 | 之前的活跃用户，一段时间未活跃（流失）今日又活跃了，就称为回流用户。此处要求统计回流用户总数。 | new_order_user_count |

`dwt_user_topic`表中存在列：`login_date_last`  ==> 用户最后一天登陆日期



**流失用户数** ：`dwt_user_topic`表：`login_date_last`  = `当前调度日期` - 7

**回流用户数**：

1. 得到`dwt_user_topic`表中**今天**分区的`login_date_last`  = `当前调度日期`  的用户id
2. 得到`dwt_user_topic`表中**昨天**分区的`login_date_last`  = `当前调度日期`- 7  的用户id
3. 步骤1和步骤2产生的结果`inner join`，得到**count结果**
4. **count结果**就是**回流用户数**

```sql
insert overwrite table ads_user_change
select * from ads_user_change
union
select
    churn.dt,
    user_churn_count,
    user_back_count
from
(
    select
        '2020-06-14' dt,
        count(*) user_churn_count
    from dwt_user_topic
    where dt='2020-06-14'
    and login_date_last=date_add('2020-06-14',-7)
)churn
join
(
    select
        '2020-06-14' dt,
        count(*) user_back_count
    from
    (
        select
            user_id,
            login_date_last
        from dwt_user_topic
        where dt='2020-06-14'
        and login_date_last='2020-06-14'
    )t1
    join
    (
        select
            user_id,
            login_date_last login_date_previous
        from dwt_user_topic
        where dt=date_add('2020-06-14',-1)
    )t2
    on t1.user_id=t2.user_id
    where datediff(login_date_last,login_date_previous)>=8
)back
on churn.dt=back.dt;
```



### 漏斗分析

用于统计用户各阶段转化情况

建表

```sql
DROP TABLE IF EXISTS ads_user_action;
CREATE EXTERNAL TABLE `ads_user_action` (
  `dt` STRING COMMENT '统计日期',
  `recent_days` BIGINT COMMENT '最近天数,1:最近1天,7:最近7天,30:最近30天',
  `home_count` BIGINT COMMENT '浏览首页人数',
  `good_detail_count` BIGINT COMMENT '浏览商品详情页人数',
  `cart_count` BIGINT COMMENT '加入购物车人数',
  `order_count` BIGINT COMMENT '下单人数',
  `payment_count` BIGINT COMMENT '支付人数'
) COMMENT '漏斗分析'
ROW FORMAT DELIMITED  FIELDS TERMINATED BY '\t'
LOCATION '/warehouse/gmall/ads/ads_user_action/';
```



SQL

实际就是从各个表算出统计值，之后join起来，得到每个环节的数据量

```sql
with
tmp_page as
(
    select
        '2020-06-14' dt,
        recent_days,
        sum(if(array_contains(pages,'home'),1,0)) home_count,
        sum(if(array_contains(pages,'good_detail'),1,0)) good_detail_count
    from
    (
        select
            recent_days,
            mid_id,
            collect_set(page_id) pages
        from
        (
            select
                dt,
                mid_id,
                page.page_id
            from dws_visitor_action_daycount lateral view explode(page_stats) tmp as page
            where dt>=date_add('2020-06-14',-29)
            and page.page_id in('home','good_detail')
        )t1 lateral view explode(Array(1,7,30)) tmp as recent_days
        where dt>=date_add('2020-06-14',-recent_days+1)
        group by recent_days,mid_id
    )t2
    group by recent_days
),
tmp_cop as
(
    select
        '2020-06-14' dt,
        recent_days,
        sum(if(cart_count>0,1,0)) cart_count,
        sum(if(order_count>0,1,0)) order_count,
        sum(if(payment_count>0,1,0)) payment_count
    from
    (
        select
            recent_days,
            user_id,
            case
                when recent_days=1 then cart_last_1d_count
                when recent_days=7 then cart_last_7d_count
                when recent_days=30 then cart_last_30d_count
            end cart_count,
            case
                when recent_days=1 then order_last_1d_count
                when recent_days=7 then order_last_7d_count
                when recent_days=30 then order_last_30d_count
            end order_count,
            case
                when recent_days=1 then payment_last_1d_count
                when recent_days=7 then payment_last_7d_count
                when recent_days=30 then payment_last_30d_count
            end payment_count
        from dwt_user_topic lateral view explode(Array(1,7,30)) tmp as recent_days
        where dt='2020-06-14'
    )t1
    group by recent_days
)
insert overwrite table ads_user_action
select * from ads_user_action
union
select
    tmp_page.dt,
    tmp_page.recent_days,
    home_count,
    good_detail_count,
    cart_count,
    order_count,
    payment_count
from tmp_page
join tmp_cop
on tmp_page.recent_days=tmp_cop.recent_days;
```



### 留存率

统计每天的1至7日留存率

1. 拿到每天的1至7日的**新增**用户数量
2. 拿到每天的1至7日的**留存**用户数量

```sql
select
    '2020-06-14',
    login_date_first  create_date, -- 创建日期
    datediff('2020-06-14',login_date_first) retention_day, -- 截至当前日期留存天数
    sum(if(login_date_last='2020-06-14',1,0)) retention_count, -- 留存用户数量
    count(*) new_user_count, -- 新增用户数量
    cast(sum(if(login_date_last='2020-06-14',1,0))/count(*)*100 as decimal(16,2)) retention_rate
from dwt_user_topic
where dt='2020-06-14'
and login_date_first>=date_add(' 020-06-14',-7)
and login_date_first<'2020-06-14'
group by login_date_first;
```



### 复购率

品牌复购率是指一段时间内重复购买某品牌的人数与购买过该品牌的人数的比值。重复购买即购买次数大于等于2，购买过即购买次数大于1。

此处要求统计最近1,7,30天的各品牌复购率。

1. **统计每个用户购买每个品牌的次数**

2. **分别统计购买次数大于1的人数和大于2的人数**

```sql
select
    '2020-06-14' dt,
    recent_days,
    tm_id,
    tm_name,
    cast(sum(if(order_count>=2,1,0))/sum(if(order_count>=1,1,0))*100 as decimal(16,2))
from
(
    select
        recent_days,
        user_id,
        tm_id,
        tm_name,
        sum(order_count) order_count
    from
    (
        select
            recent_days,
            user_id,
            sku_id,
            count(*) order_count
        from dwd_order_detail lateral view explode(Array(1,7,30)) tmp as recent_days
        where dt>=date_add('2020-06-14',-29)
        and dt>=date_add('2020-06-14',-recent_days+1)
        group by recent_days, user_id,sku_id
    )t1
    left join
    (
        select
            id,
            tm_id,
            tm_name
        from dim_sku_info
        where dt='2020-06-14'
    )t2
    on t1.sku_id=t2.id
    group by recent_days,user_id,tm_id,tm_name
)t3
group by recent_days,tm_id,tm_name;
```



## Azkaban

**自动失败重试案例**

```yaml
nodes:
  - name: JobA
    type: command
    config:
      command: sh /not_exists.sh
      retries: 3              # 重试次数
      retry.backoff: 10000    # 重试的时间间隔
```

**手动重试案例**

从界面History栏进入，找到失败的executor Id，进入，点击Prepare Execution

会自动跳过成功任务，从失败任务处开始执行

![image-20220504185930629](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220504185930629.png)

**或者**

重新手动启动这个任务链，将成功的任务disable

![image-20220504190255508](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220504190255508.png)



**jar包执行案例**

JavaProcess类型可以运行一个自定义主类方法，type类型为javaprocess，可用的配置为：

- Xms：最小堆
- Xmx：最大堆
- classpath：类路径
- java.class：要运行的Java对象，其中必须包含Main方法
- main.args：main方法的参数

将Jar包、flow文件和project文件打包上传即可

```yaml
nodes:
  - name: test_java
    type: javaprocess
    config:
      Xms: 96M
      Xmx: 200M
      java.class: com.atguigu.AzTest
```



**运行时参数案例**

1）基本原理

- 父Job将参数写入JOB_OUTPUT_PROP_FILE环境变量所指向的文件
- 子Job使用 `${jobName:param}`来获取父Job输出的参数并定义执行条件

2）支持的条件运算符：

```
==	等于
!=	不等于
>	大于
>=	大于等于
<	小于
<=	小于等于
&&  与
||	或
!	非
```

例子

JobB不需要每天都执行，而只需要每个周一执行

JobA.sh

```shell
#!/bin/bash
echo "do JobA"
wk=`date +%w`
echo "{\"wk\":$wk}" > $JOB_OUTPUT_PROP_FILE
```

JobB.sh

```shell
#!/bin/bash
echo "do JobB"
```

Azkaban任务流

```yaml
nodes:
 - name: JobA
   type: command
   config:
     command: sh JobA.sh

 - name: JobB
   type: command
   dependsOn:
     - JobA
   config:
     command: sh JobB.sh
   condition: ${JobA:wk} == 1
```



**预定义宏**

预定义宏会根据所有父Job的完成情况进行判断，再决定是否执行。可用的预定义宏如下：

1. all_success: 表示父Job全部成功才执行(默认)
2. all_done：表示父Job全部完成才执行
3. all_failed：表示父Job全部失败才执行
4. one_success：表示父Job至少一个成功才执行
5. one_failed：表示父Job至少一个失败才执行

```yaml
nodes:
 - name: JobA
   type: command
   config:
     command: sh JobA.sh

 - name: JobB
   type: command
   config:
     command: sh JobB.sh

 - name: JobC
   type: command
   dependsOn:
     - JobA
     - JobB
   config:
     command: sh JobC.sh
   condition: one_success
```

JobA与JobB，只要有一个成功，JobC就可以执行



**定时执行**

执行工作流时候，选择左下角Schedule

![img](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93wps3.jpg)



**邮件报警**

[Azkaban 邮件报警案例_Alienware^的博客-CSDN博客](https://blog.csdn.net/weixin_45417821/article/details/118192885)



**电话报警**

[【Azkaban】实现 自定义电话/微信/钉钉 报警_Kafka_Hive_Flink的博客-CSDN博客](https://blog.csdn.net/qq_43771096/article/details/107608759)



**多Executor模式下注意事项**

Azkaban多Executor模式是指，在集群中多个节点部署Executor。在这种模式下， Azkaban web Server会根据策略，选取其中一个Executor去执行任务。

如果随机指定的Executor没有任务指定的脚本就会报错

办法一：

- 指定特定的Executor（hadoop102）去执行任务
- 在MySQL中azkaban数据库executors表中，查询hadoop102上的Executor的id
- 在执行工作流程时加入useExecutor属性，如下

![img](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93wps2.jpg)



办法二：

在Executor所在所有节点部署任务所需脚本和应用。包括环境变量



**项目配置**

1）编写azkaban.project文件，内容如下

```
azkaban-flow-version: 2.0
```

2）编写gmall.flow文件，内容如下

```yaml
nodes:
  - name: mysql_to_hdfs
    type: command
    config:
     command: /home/atguigu/bin/mysql_to_hdfs.sh all ${dt}
    
  - name: hdfs_to_ods_log
    type: command
    config:
     command: /home/atguigu/bin/hdfs_to_ods_log.sh ${dt}
     
  - name: hdfs_to_ods_db
    type: command
    dependsOn: 
     - mysql_to_hdfs
    config: 
     command: /home/atguigu/bin/hdfs_to_ods_db.sh all ${dt}
  
  - name: ods_to_dim_db
    type: command
    dependsOn: 
     - hdfs_to_ods_db
    config: 
     command: /home/atguigu/bin/ods_to_dim_db.sh all ${dt}

  - name: ods_to_dwd_log
    type: command
    dependsOn: 
     - hdfs_to_ods_log
    config: 
     command: /home/atguigu/bin/ods_to_dwd_log.sh all ${dt}
    
  - name: ods_to_dwd_db
    type: command
    dependsOn: 
     - hdfs_to_ods_db
    config: 
     command: /home/atguigu/bin/ods_to_dwd_db.sh all ${dt}
    
  - name: dwd_to_dws
    type: command
    dependsOn:
     - ods_to_dim_db
     - ods_to_dwd_log
     - ods_to_dwd_db
    config:
     command: /home/atguigu/bin/dwd_to_dws.sh all ${dt}
    
  - name: dws_to_dwt
    type: command
    dependsOn:
     - dwd_to_dws
    config:
     command: /home/atguigu/bin/dws_to_dwt.sh all ${dt}
    
  - name: dwt_to_ads
    type: command
    dependsOn: 
     - dws_to_dwt
    config:
     command: /home/atguigu/bin/dwt_to_ads.sh all ${dt}
     
  - name: hdfs_to_mysql
    type: command
    dependsOn:
     - dwt_to_ads
    config:
      command: /home/atguigu/bin/hdfs_to_mysql.sh all
```

3）将azkaban.project、gmall.flow文件压缩到一个zip文件，文件名称必须是英文。

4）gmall.zip文件上传给azkaban新建的项目里面

5）配置输入dt时间参数

![img](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93wps1.jpg)



## Kylin

可以对接Hive、kafka

使用HBase存储，方便追加数据，也方便进行随机读

![image-20220504215947215](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220504215947215.png)

**1）REST Server**

REST Server是一套面向应用程序开发的入口点，旨在实现针对Kylin平台的应用开发工作。 此类应用程序可以提供查询、获取结果、触发cube构建任务、获取元数据以及获取用户权限等等。另外可以通过Restful接口实现SQL查询。

**2）查询引擎（Query Engine）**

当cube准备就绪后，查询引擎就能够获取并解析用户查询。它随后会与系统中的其它组件进行交互，从而向用户返回对应的结果。 

**3）路由器（Routing）**

在最初设计时曾考虑过将Kylin不能执行的查询引导去Hive中继续执行，但在实践后发现Hive与Kylin的速度差异过大，导致用户无法对查询的速度有一致的期望，很可能大多数查询几秒内就返回结果了，而有些查询则要等几分钟到几十分钟，因此体验非常糟糕。最后这个路由功能在发行版中默认关闭。

**4）元数据管理工具（Metadata）**

Kylin是一款元数据驱动型应用程序。元数据管理工具是一大关键性组件，用于对保存在Kylin当中的所有元数据进行管理，其中包括最为重要的cube元数据。其它全部组件的正常运作都需以元数据管理工具为基础。 Kylin的元数据存储在hbase中。 

**5）任务引擎（Cube Build Engine）**

这套引擎的设计目的在于处理所有离线任务，其中包括shell脚本、Java API以及Map Reduce任务等等。任务引擎对Kylin当中的全部任务加以管理与协调，从而确保每一项任务都能得到切实执行并解决其间出现的故障。



**兼容性问题**

Kylin和Spark引用了同jar包的不同版本，在JVM加载时，会随机加载其中一个jar包，如果不是kylin依赖的jar包，kylin运行会产生问题

**解决办法：**

1. 修改Kylin的启动脚本，冲突脚本先加载自己的
2. 修改源码，Kylin依赖和Spark的依版本赖统一

现在采用方法1来做

```shell
vim /opt/module/kylin/bin/find-spark-dependency.sh
```

原脚本

```shell
spark_dependency=`find -L $spark_home/jars -name '*.jar' ! -name '*slf4j*' ! -name '*calcite*' ! -name '*doc*' ! -name '*test*' ! -name '*sources*' ''-printf '%p:' | sed 's/:$//'`
```

改为：去掉了`jackson`和`metastore`依赖，注意保留前后空格

```shell
spark_dependency=`find -L $spark_home/jars -name '*.jar' ! -name '*slf4j*' ! -name '*calcite*' ! -name '*doc*' ! -name '*jackson*' ! -name '*metastore*' ! -name '*test*' ! -name '*sources*' ''-printf '%p:' | sed 's/:$//'`
```



启动Kylin之前，需先启动Hadoop（hdfs，yarn，jobhistoryserver）、Zookeeper、Hbase

**启动报错：**

1. 找不到依赖：检查环境变量
2. kylin web页面报404：兼容性问题脚本改错，重新修改，并重置`${dir}/cached-spark-dependency.sh`
3. 报检查Hive可用性：将`check-hive-usability.sh`中等待Hive响应时间调大

**注意**

Kylin不能处理Hive表中的复杂数据类型（Array,Map,Struct）,即便复杂类型的字段并未参与到计算之中。

如果用的表有复杂数据类型，可以依据这个表创建一个视图，这个视图丢弃掉复杂类型字段，其他的字段和原表一模一样



**任务运行报错：**

如果维度表关联的key有重复，会报错；而由于维度表是有分区的，且分区不在关联条件里面，会导致关联的key一定会有重复；

**拉链表、每日全量表**都会有这个问题

**解决办法：**同上，也是创造一个临时视图

```sql
--拉链维度表视图
create view dim_user_info_view as select * from dim_user_info where dt='9999-99-99';

--全量维度表视图（注意排除复杂数据类型字段）
create view dim_sku_info_view
as
select
    id,
    price,
    sku_name,
    sku_desc,
    weight,
    is_sale,
    spu_id,
    spu_name,
    category3_id,
    category3_name,
    category2_id,
    category2_name,
    category1_id,
    category1_name,
    tm_id,
    tm_name,
    create_time
from dim_sku_info
where dt=date_add(current_date,-1);
```





**创建Model**：创建一个星型模型，在Setting阶段设置partition时，可以不指定分区字段，而是指定业务时间字段，用于准确构建业务指标

![image-20220505223833592](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220505223833592.png)

**创建Cube**：选择真正参与计算的列，前面Model中登记的维度，以及一些优化项来构建Cube

Kylin会使用jobhistoryserver服务来保证Cube的构建步骤顺序



**每日调度**

Kylin提供了Restful API，因次我们可以将构建cube的命令写到脚本中，将脚本交给azkaban或者oozie这样的调度工具，以实现定时调度的功能

```shell
#!/bin/bash
cube_name=order_cube
do_date=`date -d '-1 day' +%F`

#获取00:00时间戳
start_date_unix=`date -d "$do_date 08:00:00" +%s`
start_date=$(($start_date_unix*1000))

#获取24:00的时间戳
stop_date=$(($start_date+86400000))

curl -X PUT -H "Authorization: Basic QURNSU46S1lMSU4=" -H 'Content-Type: application/json' -d '{"startTime":'$start_date', "endTime":'$stop_date', "buildType":"BUILD"}' http://hadoop102:7070/kylin/api/cubes/$cube_name/build
```

**注意：**写sql时候，必须保证事实表在前维度表在后

### Cube构建算法

**逐层构建算法**

构建Cube时，先计算高维结果，之后将高维结果二次聚合为低维结果

在逐层算法中，按维度数逐层减少来计算，每个层级的计算，是基于它上一层级的结果来计算的

每一轮的计算都是一个MapReduce任务，且串行执行；一个N维的Cube，至少需要N次MapReduce Job。



**优点：**

1. 此算法充分利用了MapReduce的优点，处理了中间复杂的排序和shuffle工作，故而算法代码清晰简单，易于维护；
2. 受益于Hadoop的日趋成熟，此算法非常稳定，即便是集群资源紧张时，也能保证最终能够完成。

**缺点：**

1. 当Cube有比较多维度的时候，所需要的MapReduce任务也相应增加；由于Hadoop的任务调度需要耗费额外资源，特别是集群较庞大的时候，反复递交任务造成的额外开销会相当可观；
2. 由于Mapper逻辑中并未进行聚合操作，所以每轮MR的shuffle工作量都很大，导致效率低下。
3. 对HDFS的读写操作较多：由于每一层计算的输出会用做下一层计算的输入，这些Key-Value需要写到HDFS上；当所有计算都完成后，Kylin还需要额外的一轮任务将这些文件转成HBase的HFile格式，以导入到HBase中去；

总体而言，该算法的效率较低，尤其是当Cube维度数较大的时候。



**快速构建算法**

也被称作“逐段”(By Segment) 或“逐块”(By Split) 算法，从1.5.x开始引入该算法，该算法的主要思想是，每个Mapper将其所分配到的数据块，计算成一个完整的小Cube 段（包含所有Cuboid）。每个Mapper将计算完的Cube段输出给Reducer做合并，生成大Cube，也就是最终结果。

在Map端从高维到底维全算完，之后Reduce端合并Map端数据

对内存要求较高

Kylin会自动选择算法



### Cube存储原理

**维度字典：**构建维度字段，为每一个维度值定义一个编码，参与计算的时候，就只对编码分组计算

![image-20220506213721631](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220506213721631.png)

**HBase存储**

Rowkey：Cuboid+维度值 

在HBase底层以16进制存储，Cuboid和维度值之间有分隔符，多个维度值之间也有分隔符

![image-20220506214137322](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220506214137322.png)



### Kylin优化

**衍生维度**

用于加快计算速度，减少cuboid的搭建数量

将维度表上的非主键维度排除掉，并使用维度表的主键（其实是事实表上相应的外键）来替代它们

如果选择的维度是衍生维度，这个维度不参与计算，所选的衍生维度所在的维度表的主键才会参与计算；而真正参与计算的也不是维度表的主键，而是事实表与该维度表的join key，所以就是事实表的维度外键参与计算；

Kylin会在底层记录维度表主键与维度表其他维度之间的映射关系，以便在查询时能够动态地将维度表的主键“翻译”成这些非主键维度，并进行实时聚合。

![image-20220507203223522](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220507203223522.png)

**产生问题：**临时查询时候，会将事实表外键转为实际维度值，并进行二次聚合，如果从维度表主键到某个维度表维度所需要的聚合工作量非常大，则不建议使用衍生维度。

而如果维度值的去重个数和维度表主键个数差距不大（10倍以内），可以使用衍生维度优化



**聚合组**

其实也是剪枝优化，去掉一些无用维度的聚合

**强制维度（Mandatory**）：如果一个维度被定义为强制维度，那么这个分组产生的所有Cuboid中每一个Cuboid都会包含该维度。每个分组中都可以有0个、1个或多个强制维度。如果根据这个分组的业务逻辑，则相关的查询一定会在过滤条件或分组条件中，因此可以在该分组中把该维度设置为强制维度。

![image-20220507211718089](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220507211718089.png)

**层级维度（Hierarchy）**：每个层级包含两个或更多个维度，低层级维度不可以单独出现。如下图，B维度无法单独出现

![image-20220507211814788](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220507211814788.png)

**联合维度（Joint）**：每个联合中包含两个或更多个维度，如果某些列形成一个联合，那么在该分组产生的任何Cuboid中，这些联合维度要么一起出现，要么都不出现。

![image-20220507212737380](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220507212737380.png)



操作界面

![image-20220507212831400](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220507212831400.png)



**Rowkey优化**

Kylin会把所有的维度按照顺序组合成一个完整的Rowkey，并且按照这个Rowkey升序排列Cuboid中所有的行。

设计良好的Rowkey将更有效地完成数据的查询过滤和定位，减少IO次数，提高查询速度，维度在rowkey中的次序，对查询性能有显著的影响。

1. **被用作过滤的维度放在前边**，优化查询
2. **基数大的维度放在基数小的维度前边**，优化计算，适合逐层降维计算

如下图，D的基数比C的基数小，数据量也会更小，使用ABD去聚合AB维度的值，会更快一些；

由于Kylin优先去选择cuboid小的去降维聚合，所以我们最好手动去把基数大的维度前调

![image-20220507211006337](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220507211006337.png)

Rowkey调整界面：

![image-20220507211410324](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220507211410324.png)



## Presto

Presto是一个开源的分布式SQL查询引擎，支持GB到PB的秒级查询

**注意：**虽然Presto可以解析SQL，但它不是标准的数据库，不是MySQL、Oracle的替代品，也不能处理在线事务

可以对接丰富的数据源

MapReduce慢在磁盘IO和网络IO，而Presto从内存到内存，所有stage拥有管道，生产执行计划，直接从头执行到尾，省去stage阶段之间等待时间；

**最重要优点**：可以对接多个数据源，并联表查询，类似于SparkSQL临时表

**缺点**：对于普通聚合，会边读表边计算，边清理内存，实际消耗内存比较少；但是联表查（类似shuffle）会产生大量临时数据，速度会变慢；

针对这个缺点，可以Hive先join完之后，Presto操作这个大宽表去即席查询

**注意：**Presto SQL不可以写分号，会报错



**Presto、Impala性能比较**

测试结论：Impala性能稍领先于Presto，但是Presto在数据源支持上非常丰富，包括Hive、图数据库、传统关系型数据库、Redis等。



### Lzo不兼容问题

 [Presto实现原理和美团的使用实践 - 美团技术团队 (meituan.com)](https://tech.meituan.com/2014/06/16/presto.html)

由于本项目不读ods表，所以不修改



### Presto优化

**数据存储**

1. 合理设置分区
2. 使用**列式存储**，Presto对ORC文件读取做了特定优化，相对于Parquet，Presto对ORC支持更好
3. **数据压缩**，可以减少节点间数据传输对IO带宽压力，对于即席查询需要快速解压，建议采用Snappy压缩

**SQL编写**

1. 只选择使用的字段

2. 尽量使用分区过滤

3. Group By语句优化，合理安排Group by语句中字段顺序，将Group By语句中字段按照每个字段distinct数据多少进行降序排列

4. Order by时使用Limit，如果是查询Top N或者Bottom N，使用limit可减少排序计算和内存压力

5. **使用Join语句时将大表放在左边**；Presto中join的默认算法是broadcast join，即将join左边的表分割到多个worker，然后将join右边的表数据整个复制一份发送到每个worker进行计算。如果右边的表数据量太大，则可能会报内存溢出错误；

   如果是大表join大表，就会转为hash join

```
[GOOD] SELECT ... FROM large_table l join small_table s on l.id = s.id
[BAD]  SELECT ... FROM small_table s join large_table l on l.id = s.id
```

### 注意事项

**字段名引用**：避免和关键字冲突：MySQL对字段加反引号、Presto对字段加双引号分割

**时间函数**：对于Timestamp，需要进行比较的时候，需要添加Timestamp关键字，而MySQL中对Timestamp可以直接进行比较

```
/*MySQL的写法*/
SELECT t FROM a WHERE t > '2017-01-01 00:00:00'; 

/*Presto中的写法*/
SELECT t FROM a WHERE t > timestamp '2017-01-01 00:00:00';
```

**不支持INSERT OVERWRITE语法**，Presto中不支持insert overwrite语法，只能先delete，然后insert into

**Parquet格式**：Presto目前支持Parquet格式，支持查询，但不支持insert



## Zabbix

Zabbix是一款能够监控各种网络参数以及服务器健康性和完整性的软件。Zabbix使用灵活的通知机制，允许用户为几乎任何事件配置基于邮件的告警。这样可以快速反馈服务器的问题。基于已存储的数据，Zabbix提供了出色的报告和数据可视化功能。



## Kerberos

Kerberos是一种计算机网络认证协议，用来在非安全网络中，对个人通信以安全的手段进行**身份认证**

### Kerberos术语

Kerberos中有以下一些概念需要了解：

1. KDC（Key Distribute Center）：密钥分发中心，负责存储用户信息，管理发放票据。
2. Realm：Kerberos所管理的一个领域或范围，称之为一个Realm。
3. Rrincipal：Kerberos所管理的一个用户或者一个服务，可以理解为Kerberos中保存的一个账号，其格式通常如下：primary/instance@realm（一般是 账号/主机名@公司域账号）
4. keytab：Kerberos中的用户认证，可通过密码或者密钥文件证明身份，keytab指密钥文件。



原理

![image-20220508192504980](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220508192504980.png)



### 创建hadoop用户组

linux创建用户和用户组

```shell
groupadd hadoop

useradd hdfs -g hadoop
echo hdfs | passwd --stdin  hdfs
useradd yarn -g hadoop
echo yarn | passwd --stdin yarn
useradd mapred -g hadoop
echo mapred | passwd --stdin mapred
```



**为每个服务创建服务主体**

主体格式如下：ServiceName/HostName@REALM

为服务创建的主体，需要通过密钥文件keytab文件进行认证，故需为各服务准备一个安全的路径用来存储keytab文件。

```shell
mkdir /etc/security/keytab/
chown -R root:hadoop /etc/security/keytab/
chmod 770 /etc/security/keytab/
```

为执行创建主体的语句，需登录Kerberos 数据库客户端，登录之前需先使用Kerberos的管理员用户进行认证，执行以下命令并根据提示输入密码。

**在所有节点创建keytab文件目录**

```shell
[root@hadoop102 ~]# mkdir /etc/security/keytab/
[root@hadoop102 ~]# chown -R root:hadoop /etc/security/keytab/
[root@hadoop102 ~]# chmod 770 /etc/security/keytab/

[root@hadoop103 ~]# mkdir /etc/security/keytab/
[root@hadoop103 ~]# chown -R root:hadoop /etc/security/keytab/
[root@hadoop103 ~]# chmod 770 /etc/security/keytab/

[root@hadoop104 ~]# mkdir /etc/security/keytab/
[root@hadoop104 ~]# chown -R root:hadoop /etc/security/keytab/
[root@hadoop104 ~]# chmod 770 /etc/security/keytab/
```

**以下命令在hadoop102节点执行**

NameNode

```shell
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey nn/hadoop102"
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/nn.service.keytab nn/hadoop102"
```

DataNode

```shell
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey dn/hadoop102"
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/dn.service.keytab dn/hadoop102"
```

NodeManager

```shell
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey nm/hadoop102"
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/nm.service.keytab nm/hadoop102"
```

JobHistory Server

```shell
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey jhs/hadoop102"
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/jhs.service.keytab jhs/hadoop102"
```

Web UI

```shell
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey HTTP/hadoop102"
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/spnego.service.keytab HTTP/hadoop102"
```

**以下命令在hadoop103执行**

ResourceManager

```shell
[root@hadoop103 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey rm/hadoop103"
[root@hadoop103 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/rm.service.keytab rm/hadoop103"
```

DataNode

```shell
[root@hadoop103 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey dn/hadoop103"
[root@hadoop103 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/dn.service.keytab dn/hadoop103"
```

NodeManager

```shell
[root@hadoop103 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey nm/hadoop103"
[root@hadoop103 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/nm.service.keytab nm/hadoop103"
```

Web UI

```shell
[root@hadoop103 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey HTTP/hadoop103"
[root@hadoop103 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/spnego.service.keytab HTTP/hadoop103"
```

**以下命令在hadoop104执行**

DataNode

```shell
[root@hadoop104 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey dn/hadoop104"
[root@hadoop104 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/dn.service.keytab dn/hadoop104"
```

Secondary NameNode

```shell
[root@hadoop104 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey sn/hadoop104"
[root@hadoop104 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/sn.service.keytab sn/hadoop104"
```

NodeManager

```shell
[root@hadoop104 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey nm/hadoop104"
[root@hadoop104 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/nm.service.keytab nm/hadoop104"
```

Web UI

```shell
[root@hadoop104 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey HTTP/hadoop104"
[root@hadoop104 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/spnego.service.keytab HTTP/hadoop104"
```

**修改所有节点keytab文件的所有者和访问权限**

```shell
[root@hadoop102 ~]# chown -R root:hadoop /etc/security/keytab/
[root@hadoop102 ~]# chmod 660 /etc/security/keytab/*

[root@hadoop103 ~]# chown -R root:hadoop /etc/security/keytab/
[root@hadoop103 ~]# chmod 660 /etc/security/keytab/*

[root@hadoop104 ~]# chown -R root:hadoop /etc/security/keytab/
[root@hadoop104 ~]# chmod 660 /etc/security/keytab/*
```



### 修改配置文件

core-site.xml 

```xml
[root@hadoop102 ~]# vim /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml
增加以下内容

<!-- Kerberos主体到系统用户的映射机制 -->
<property>
  <name>hadoop.security.auth_to_local.mechanism</name>
  <value>MIT</value>
</property>

<!-- Kerberos主体到系统用户的具体映射规则 -->
<property>
  <name>hadoop.security.auth_to_local</name>
  <value>
    RULE:[2:$1/$2@$0]([ndj]n\/.*@EXAMPLE\.COM)s/.*/hdfs/
    RULE:[2:$1/$2@$0]([rn]m\/.*@EXAMPLE\.COM)s/.*/yarn/
    RULE:[2:$1/$2@$0](jhs\/.*@EXAMPLE\.COM)s/.*/mapred/
    DEFAULT
  </value>
</property>

<!-- 启用Hadoop集群Kerberos安全认证 -->
<property>
  <name>hadoop.security.authentication</name>
  <value>kerberos</value>
</property>

<!-- 启用Hadoop集群授权管理 -->
<property>
  <name>hadoop.security.authorization</name>
  <value>true</value>
</property>

<!-- Hadoop集群间RPC通讯设为仅认证模式 -->
<property>
  <name>hadoop.rpc.protection</name>
  <value>authentication</value>
</property>
```



### HTTPS配置

**HTTPS验证流程**

1. 客户端发送请求给服务端
2. 服务端返回证书(证书包含非对称加密公钥)
3. 客户端验证证书，验证成功后，产生一个对称加密密钥对，使用服务端公钥加密生成的密钥
4. 客户端发送加密后的密钥发给服务端
5. 服务端用自己的私钥解密，得到客户端的密钥
6. 后续通讯均使用对称密钥加密

**术语**

Keytool是java数据证书的管理工具，使用户能够管理自己的公/私钥对及相关证书。

-keystore  指定密钥库的名称及位置(产生的各类信息将存在.keystore文件中)

-genkey(或者-genkeypair)   生成密钥对

-alias  为生成的密钥对指定别名，如果没有默认是mykey

-keyalg 指定密钥的算法 RSA/DSA 默认是DSA



**生成keystore的密码及相应信息的密钥库**

```shell
[root@hadoop102 ~]# keytool -keystore /etc/security/keytab/keystore -alias jetty -genkey -keyalg RSA
```

**修改keystore文件的所有者和访问权限**

```shell
[root@hadoop102 ~]# chown -R root:hadoop /etc/security/keytab/keystore
[root@hadoop102 ~]# chmod 660 /etc/security/keytab/keystore
```

**将该证书分发到集群中的每台节点的相同路径**

```shell
[root@hadoop102 ~]# xsync /etc/security/keytab/keystore
```

**修改hadoop配置文件ssl-server.xml.example**

```shell
[root@hadoop102 ~]# mv $HADOOP_HOME/etc/hadoop/ssl-server.xml.example $HADOOP_HOME/etc/hadoop/ssl-server.xml
```

`vim $HADOOP_HOME/etc/hadoop/ssl-server.xml`

```shell
<!-- SSL密钥库路径 -->
<property>
  <name>ssl.server.keystore.location</name>
  <value>/etc/security/keytab/keystore</value>
</property>

<!-- SSL密钥库密码 -->
<property>
  <name>ssl.server.keystore.password</name>
  <value>123456</value>
</property>

<!-- SSL可信任密钥库路径 -->
<property>
  <name>ssl.server.truststore.location</name>
  <value>/etc/security/keytab/keystore</value>
</property>

<!-- SSL密钥库中密钥的密码 -->
<property>
  <name>ssl.server.keystore.keypassword</name>
  <value>123456</value>
</property>

<!-- SSL可信任密钥库密码 -->
<property>
  <name>ssl.server.truststore.password</name>
  <value>123456</value>
</property>
```

分发文件

```shell
 xsync $HADOOP_HOME/etc/hadoop/ssl-server.xml
```



### 配置YARN

修改**所有节点**的container-executor所有者和权限，要求其所有者为root，所有组为hadoop（启动NodeManger的yarn用户的所属组），权限为6050。其默认路径为`$HADOOP_HOME/bin`

```shell
[root@hadoop102 ~]# chown root:hadoop /opt/module/hadoop-3.1.3/bin/container-executor
[root@hadoop102 ~]# chmod 6050 /opt/module/hadoop-3.1.3/bin/container-executor

[root@hadoop103 ~]# chown root:hadoop /opt/module/hadoop-3.1.3/bin/container-executor
[root@hadoop103 ~]# chmod 6050 /opt/module/hadoop-3.1.3/bin/container-executor

[root@hadoop104 ~]# chown root:hadoop /opt/module/hadoop-3.1.3/bin/container-executor
[root@hadoop104 ~]# chmod 6050 /opt/module/hadoop-3.1.3/bin/container-executor
```

修改所有节点的container-executor.cfg文件的所有者和权限，要求该文件及其所有的上级目录的所有者均为root，所有组为hadoop（启动NodeManger的yarn用户的所属组），权限为400。其默认路径为`$HADOOP_HOME/etc/hadoop`

```shell
[root@hadoop102 ~]# chown root:hadoop /opt/module/hadoop-3.1.3/etc/hadoop/container-executor.cfg
[root@hadoop102 ~]# chown root:hadoop /opt/module/hadoop-3.1.3/etc/hadoop
[root@hadoop102 ~]# chown root:hadoop /opt/module/hadoop-3.1.3/etc
[root@hadoop102 ~]# chown root:hadoop /opt/module/hadoop-3.1.3
[root@hadoop102 ~]# chown root:hadoop /opt/module
[root@hadoop102 ~]# chmod 400 /opt/module/hadoop-3.1.3/etc/hadoop/container-executor.cfg

[root@hadoop103 ~]# chown root:hadoop /opt/module/hadoop-3.1.3/etc/hadoop/container-executor.cfg
[root@hadoop103 ~]# chown root:hadoop /opt/module/hadoop-3.1.3/etc/hadoop
[root@hadoop103 ~]# chown root:hadoop /opt/module/hadoop-3.1.3/etc
[root@hadoop103 ~]# chown root:hadoop /opt/module/hadoop-3.1.3
[root@hadoop103 ~]# chown root:hadoop /opt/module
[root@hadoop103 ~]# chmod 400 /opt/module/hadoop-3.1.3/etc/hadoop/container-executor.cfg

[root@hadoop104 ~]# chown root:hadoop /opt/module/hadoop-3.1.3/etc/hadoop/container-executor.cfg
[root@hadoop104 ~]# chown root:hadoop /opt/module/hadoop-3.1.3/etc/hadoop
[root@hadoop104 ~]# chown root:hadoop /opt/module/hadoop-3.1.3/etc
[root@hadoop104 ~]# chown root:hadoop /opt/module/hadoop-3.1.3
[root@hadoop104 ~]# chown root:hadoop /opt/module
[root@hadoop104 ~]# chmod 400 /opt/module/hadoop-3.1.3/etc/hadoop/container-executor.cfg
```

修改`$HADOOP_HOME/etc/hadoop/container-executor.cfg`

```shell
[root@hadoop102 ~]# vim $HADOOP_HOME/etc/hadoop/container-executor.cfg
```

内容如下

```
yarn.nodemanager.linux-container-executor.group=hadoop
banned.users=hdfs,yarn,mapred
min.user.id=1000
allowed.system.users=
feature.tc.enabled=false
```

修改`$HADOOP_HOME/etc/hadoop/yarn-site.xml`文件

```shell
vim $HADOOP_HOME/etc/hadoop/yarn-site.xml
```

增加以下内容

```xml
<!-- 配置Node Manager使用LinuxContainerExecutor管理Container -->
<property>
  <name>yarn.nodemanager.container-executor.class</name>
  <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
</property>

<!-- 配置Node Manager的启动用户的所属组 -->
<property>
  <name>yarn.nodemanager.linux-container-executor.group</name>
  <value>hadoop</value>
</property>

<!-- LinuxContainerExecutor脚本路径 -->
<property>
  <name>yarn.nodemanager.linux-container-executor.path</name>
  <value>/opt/module/hadoop-3.1.3/bin/container-executor</value>
</property>
```

分发`container-executor.cfg`和`yarn-site.xml`文件

```
[root@hadoop102 ~]# xsync $HADOOP_HOME/etc/hadoop/container-executor.cfg
[root@hadoop102 ~]# xsync $HADOOP_HOME/etc/hadoop/yarn-site.xml
```



### Hadoop集群启动

使用特定用户启动服务之前，需要将特定文件访问路径授权给它们，防止因为没有访问某些目录的权限导致服务启动失败

**修改特定本地路径权限**

1）$HADOOP_LOG_DIR（所有节点）

该变量位于hadoop-env.sh文件，默认值为 ${HADOOP_HOME}/logs

```shell
[root@hadoop102 ~]# chown hdfs:hadoop /opt/module/hadoop-3.1.3/logs/
[root@hadoop102 ~]# chmod 775 /opt/module/hadoop-3.1.3/logs/

[root@hadoop103 ~]# chown hdfs:hadoop /opt/module/hadoop-3.1.3/logs/
[root@hadoop103 ~]# chmod 775 /opt/module/hadoop-3.1.3/logs/

[root@hadoop104 ~]# chown hdfs:hadoop /opt/module/hadoop-3.1.3/logs/
[root@hadoop104 ~]# chmod 775 /opt/module/hadoop-3.1.3/logs/
```

2）dfs.namenode.name.dir（NameNode节点）

该参数位于hdfs-site.xml文件，默认值为file://${hadoop.tmp.dir}/dfs/name

```shell
[root@hadoop102 ~]# chown -R hdfs:hadoop /opt/module/hadoop-3.1.3/data/dfs/name/
[root@hadoop102 ~]# chmod 700 /opt/module/hadoop-3.1.3/data/dfs/name/
```

3）dfs.datanode.data.dir（DataNode节点）

该参数为于hdfs-site.xml文件，默认值为file://${hadoop.tmp.dir}/dfs/data

```shell
[root@hadoop102 ~]# chown -R hdfs:hadoop /opt/module/hadoop-3.1.3/data/dfs/data/
[root@hadoop102 ~]# chmod 700 /opt/module/hadoop-3.1.3/data/dfs/data/

[root@hadoop103 ~]# chown -R hdfs:hadoop /opt/module/hadoop-3.1.3/data/dfs/data/
[root@hadoop103 ~]# chmod 700 /opt/module/hadoop-3.1.3/data/dfs/data/

[root@hadoop104 ~]# chown -R hdfs:hadoop /opt/module/hadoop-3.1.3/data/dfs/data/
[root@hadoop104 ~]# chmod 700 /opt/module/hadoop-3.1.3/data/dfs/data/
```



4）dfs.namenode.checkpoint.dir（SecondaryNameNode节点）

该参数位于hdfs-site.xml文件，默认值为file://${hadoop.tmp.dir}/dfs/namesecondary

```shell
[root@hadoop104 ~]# chown -R hdfs:hadoop /opt/module/hadoop-3.1.3/data/dfs/namesecondary/
[root@hadoop104 ~]# chmod 700 /opt/module/hadoop-3.1.3/data/dfs/namesecondary/
```

5）yarn.nodemanager.local-dirs（NodeManager节点）

该参数位于yarn-site.xml文件，默认值为file://${hadoop.tmp.dir}/nm-local-dir

```shell
[root@hadoop102 ~]# chown -R yarn:hadoop /opt/module/hadoop-3.1.3/data/nm-local-dir/
[root@hadoop102 ~]# chmod -R 775 /opt/module/hadoop-3.1.3/data/nm-local-dir/

[root@hadoop103 ~]# chown -R yarn:hadoop /opt/module/hadoop-3.1.3/data/nm-local-dir/
[root@hadoop103 ~]# chmod -R 775 /opt/module/hadoop-3.1.3/data/nm-local-dir/

[root@hadoop104 ~]# chown -R yarn:hadoop /opt/module/hadoop-3.1.3/data/nm-local-dir/
[root@hadoop104 ~]# chmod -R 775 /opt/module/hadoop-3.1.3/data/nm-local-dir/
```



6）yarn.nodemanager.log-dirs（NodeManager节点）

该参数位于yarn-site.xml文件，默认值为$HADOOP_LOG_DIR/userlogs

```shell
[root@hadoop102 ~]# chown yarn:hadoop /opt/module/hadoop-3.1.3/logs/userlogs/
[root@hadoop102 ~]# chmod 775 /opt/module/hadoop-3.1.3/logs/userlogs/

[root@hadoop103 ~]# chown yarn:hadoop /opt/module/hadoop-3.1.3/logs/userlogs/
[root@hadoop103 ~]# chmod 775 /opt/module/hadoop-3.1.3/logs/userlogs/

[root@hadoop104 ~]# chown yarn:hadoop /opt/module/hadoop-3.1.3/logs/userlogs/
[root@hadoop104 ~]# chmod 775 /opt/module/hadoop-3.1.3/logs/userlogs/
```



**启动HDFS**

单点启动

（1）启动NameNode

```
[root@hadoop102 ~]# sudo -i -u hdfs hdfs --daemon start namenode
```

（2）启动DataNode

```
[root@hadoop102 ~]# sudo -i -u hdfs hdfs --daemon start datanode

[root@hadoop103 ~]# sudo -i -u hdfs hdfs --daemon start datanode

[root@hadoop104 ~]# sudo -i -u hdfs hdfs --daemon start datanode
```

（3）启动SecondaryNameNode

```
[root@hadoop104 ~]# sudo -i -u hdfs hdfs --daemon start secondarynamenode
```

说明：

-  -i：重新加载环境变量
-  -u：以特定用户的身份执行后续命令



群起

- 在主节点（hadoop102）配置hdfs用户到所有节点的免密登录
- 修改主节点（hadoop102）节点的$HADOOP_HOME/sbin/start-dfs.sh脚本，在顶部增加以下环境变量

```shell
# 用于声明启动用户
HDFS_DATANODE_USER=hdfs
HDFS_NAMENODE_USER=hdfs
HDFS_SECONDARYNAMENODE_USER=hdfs
```

同时 ，$HADOOP_HOME/sbin/stop-dfs.sh也需在顶部增加上述环境变量

- 以root用户执行群起脚本，即可启动HDFS集群

```shell
start-dfs.sh
```



**修改HDFS特定路径访问权限**

| hdfs | /                                          | hdfs:hadoop   | drwxr-xr-x  |
| ---- | ------------------------------------------ | ------------- | ----------- |
| hdfs | /tmp                                       | hdfs:hadoop   | drwxrwxrwxt |
| hdfs | /user                                      | hdfs:hadoop   | drwxrwxr-x  |
| hdfs | yarn.nodemanager.remote-app-log-dir        | yarn:hadoop   | drwxrwxrwxt |
| hdfs | mapreduce.jobhistory.intermediate-done-dir | mapred:hadoop | drwxrwxrwxt |
| hdfs | mapreduce.jobhistory.done-dir              | mapred:hadoop | drwxrwx---  |

若上述路径不存在，需手动创建

1）创建hdfs/hadoop主体，执行以下命令并按照提示输入密码

```
[root@hadoop102 ~]# kadmin.local -q "addprinc hdfs/hadoop"
```

2）认证hdfs/hadoop主体，执行以下命令并按照提示输入密码

```
[root@hadoop102 ~]# kinit hdfs/hadoop
```

3）按照上述要求修改指定路径的所有者和权限

（1）修改/、/tmp、/user路径

```
[root@hadoop102 ~]# hadoop fs -chown hdfs:hadoop / /tmp /user

[root@hadoop102 ~]# hadoop fs -chmod 755 /

[root@hadoop102 ~]# hadoop fs -chmod 1777 /tmp

[root@hadoop102 ~]# hadoop fs -chmod 775 /user
```

（2）参数yarn.nodemanager.remote-app-log-dir位于yarn-site.xml文件，默认值/tmp/logs

```
[root@hadoop102 ~]# hadoop fs -chown yarn:hadoop /tmp/logs

[root@hadoop102 ~]# hadoop fs -chmod 1777 /tmp/logs
```

（3）参数mapreduce.jobhistory.intermediate-done-dir位于mapred-site.xml文件，默认值为/tmp/hadoop-yarn/staging/history/done_intermediate，需保证该路径的所有上级目录（除/tmp）的所有者均为mapred，所属组为hadoop，权限为770

```
[root@hadoop102 ~]# hadoop fs -chown -R mapred:hadoop /tmp/hadoop-yarn/staging/history/done_intermediate
[root@hadoop102 ~]# hadoop fs -chmod -R 1777 /tmp/hadoop-yarn/staging/history/done_intermediate

[root@hadoop102 ~]# hadoop fs -chown mapred:hadoop /tmp/hadoop-yarn/staging/history/
[root@hadoop102 ~]# hadoop fs -chown mapred:hadoop /tmp/hadoop-yarn/staging/
[root@hadoop102 ~]# hadoop fs -chown mapred:hadoop /tmp/hadoop-yarn/

[root@hadoop102 ~]# hadoop fs -chmod 770 /tmp/hadoop-yarn/staging/history/
[root@hadoop102 ~]# hadoop fs -chmod 770 /tmp/hadoop-yarn/staging/
[root@hadoop102 ~]# hadoop fs -chmod 770 /tmp/hadoop-yarn/
```

（4）参数mapreduce.jobhistory.done-dir位于mapred-site.xml文件，默认值为/tmp/hadoop-yarn/staging/history/done，需保证该路径的所有上级目录（除/tmp）的所有者均为mapred，所属组为hadoop，权限为770

```
[root@hadoop102 ~]# hadoop fs -chown -R mapred:hadoop /tmp/hadoop-yarn/staging/history/done
[root@hadoop102 ~]# hadoop fs -chmod -R 750 /tmp/hadoop-yarn/staging/history/done

[root@hadoop102 ~]# hadoop fs -chown mapred:hadoop /tmp/hadoop-yarn/staging/history/
[root@hadoop102 ~]# hadoop fs -chown mapred:hadoop /tmp/hadoop-yarn/staging/
[root@hadoop102 ~]# hadoop fs -chown mapred:hadoop /tmp/hadoop-yarn/

[root@hadoop102 ~]# hadoop fs -chmod 770 /tmp/hadoop-yarn/staging/history/
[root@hadoop102 ~]# hadoop fs -chmod 770 /tmp/hadoop-yarn/staging/ 
[root@hadoop102 ~]# hadoop fs -chmod 770 /tmp/hadoop-yarn/
```



**启动Yarn**

需要先配置yarn用户ssh免密登陆

1.单点启动

启动ResourceManager

```
[root@hadoop103 ~]# sudo -i -u yarn yarn --daemon start resourcemanager
```

启动NodeManager

```
[root@hadoop102 ~]# sudo -i -u yarn yarn --daemon start nodemanager

[root@hadoop103 ~]# sudo -i -u yarn yarn --daemon start nodemanager

[root@hadoop104 ~]# sudo -i -u yarn yarn --daemon start nodemanager
```

2.群起
1）在Yarn主节点（hadoop103）配置yarn用户到所有节点的免密登录。
2）修改主节点（hadoop103）的$HADOOP_HOME/sbin/start-yarn.sh，在顶部增加以下环境变量。

```
[root@hadoop103 ~]# vim $HADOOP_HOME/sbin/start-yarn.sh
在顶部增加如下内容

YARN_RESOURCEMANAGER_USER=yarn
YARN_NODEMANAGER_USER=yarn
```

注：stop-yarn.sh也需在顶部增加上述环境变量才可使用。

3）以root用户执行$HADOOP_HOME/sbin/start-yarn.sh脚本即可启动yarn集群。

```
[root@hadoop103 ~]# start-yarn.sh
```

3.访问Yarn web页面

访问地址为http://hadoop103:8088 



**启动HistoryServer**

1.启动历史服务器

```
[root@hadoop102 ~]# sudo -i -u mapred mapred --daemon start historyserver
```

2.查看历史服务器web页面

访问地址为http://hadoop102:19888 



### 安全集群使用说明

**用户要求**

以下使用说明均基于普通用户，安全集群对用户有以下要求：

1. 集群中的每个节点都需要创建该用户
2. 该用户需要属于hadoop用户组
3. 需要创建该用户对应的Kerberos主体

2.实操

此处以atguigu用户为例，具体操作如下 

1. 创建用户（存在可跳过），须在所有节点执行

```
[root@hadoop102 ~]# useradd atguigu
[root@hadoop102 ~]# echo atguigu | passwd --stdin atguigu

[root@hadoop103 ~]# useradd atguigu
[root@hadoop103 ~]# echo atguigu | passwd --stdin atguigu

[root@hadoop104 ~]# useradd atguigu
[root@hadoop104 ~]# echo atguigu | passwd --stdin atguigu
```

2. 加入hadoop组，须在所有节点执行

```
[root@hadoop102 ~]# usermod -a -G hadoop atguigu
[root@hadoop103 ~]# usermod -a -G hadoop atguigu
[root@hadoop104 ~]# usermod -a -G hadoop atguigu
```

3. 创建主体

```
[root@hadoop102 ~]# kadmin -p admin/admin -wadmin -q"addprinc -pw atguigu atguigu"
```



**Shell命令访问HDFS**

先认证用户

```
[atguigu@hadoop102 ~]$ kinit atguigu
```

执行命令

```
[atguigu@hadoop102 ~]$ hadoop fs -ls /
```



**提交MR任务**

认证

```
[atguigu@hadoop102 ~]$ kinit atguigu
```

执行任务

```
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 1 1
```



### Hive认证

要求Hadoop集群先完成Kerberos认证

**创建Hive系统用户和Kerberos主体**

创建系统用户

```
[root@hadoop102 ~]# useradd hive -g hadoop
[root@hadoop102 ~]# echo hive | passwd --stdin hive

[root@hadoop103 ~]# useradd hive -g hadoop
[root@hadoop103 ~]# echo hive | passwd --stdin hive

[root@hadoop104 ~]# useradd hive -g hadoop
[root@hadoop104 ~]# echo hive | passwd --stdin hive
```

创建Kerberos主体并生成keytab文件

```
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey hive/hadoop102"
```

在Hive所部署的节点生成keytab文件

```
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/hive.service.keytab hive/hadoop102"
```

修改keytab文件所有者和访问权限

```
[root@hadoop102 ~]# chown -R root:hadoop /etc/security/keytab/
[root@hadoop102 ~]# chmod 660 /etc/security/keytab/hive.service.keytab
```



**修改Hive配置**

修改$HIVE_HOME/conf/hive-site.xml文件

```xml
<!-- HiveServer2启用Kerberos认证 -->
<property>
    <name>hive.server2.authentication</name>
    <value>kerberos</value>
</property>

<!-- HiveServer2服务的Kerberos主体 -->
<property>
    <name>hive.server2.authentication.kerberos.principal</name>
    <value>hive/hadoop102@EXAMPLE.COM</value>
</property>

<!-- HiveServer2服务的Kerberos密钥文件 -->
<property>
    <name>hive.server2.authentication.kerberos.keytab</name>
    <value>/etc/security/keytab/hive.service.keytab</value>
</property>

<!-- Metastore启动认证 -->
<property>
    <name>hive.metastore.sasl.enabled</name>
    <value>true</value>
</property>
<!-- Metastore Kerberos密钥文件 -->
<property>
    <name>hive.metastore.kerberos.keytab.file</name>
    <value>/etc/security/keytab/hive.service.keytab</value>
</property>
<!-- Metastore Kerberos主体 -->
<property>
    <name>hive.metastore.kerberos.principal</name>
    <value>hive/hadoop102@EXAMPLE.COM</value>
</property>
```

修改$HADOOP_HOME/etc/hadoop/core-site.xml文件

**删除**以下参数，把代理用户删掉，防止所有用户都共享atguigu用户权限

```
<property>
    <name>hadoop.http.staticuser.user</name>
    <value>atguigu</value>
</property>

<property>
    <name>hadoop.proxyuser.atguigu.hosts</name>
    <value>*</value>
</property>

<property>
    <name>hadoop.proxyuser.atguigu.groups</name>
    <value>*</value>
</property>

<property>
    <name>hadoop.proxyuser.atguigu.users</name>
    <value>*</value>
</property>
```

**增加**以下参数，把代理用户改成提交任务用户

```xml
<property>
    <name>hadoop.proxyuser.hive.hosts</name>
    <value>*</value>
</property>

<property>
    <name>hadoop.proxyuser.hive.groups</name>
    <value>*</value>
</property>

<property>
    <name>hadoop.proxyuser.hive.users</name>
    <value>*</value>
</property>
```

分发配置core-site.xml文件

```
[root@hadoop102 ~]# xsync $HADOOP_HOME/etc/hadoop/core-site.xml
```

重启hadoop

```
[root@hadoop102 ~]# stop-dfs.sh
[root@hadoop103 ~]# stop-yarn.sh

[root@hadoop102 ~]# start-dfs.sh
[root@hadoop103 ~]# start-yarn.sh
```

Hive用户启动hiveserver2

```
[root@hadoop102 ~]# sudo -i -u hive hiveserver2
```

**Hive Kerberos认证使用**

**beeline客户端**

1.认证，执行以下命令，并按照提示输入密码

```
[atguigu@hadoop102 ~]$ kinit atguigu
```

使用beeline客户端连接hiveserver2

```
[atguigu@hadoop102 ~]$ beeline
```

使用如下url进行连接

```
> !connect jdbc:hive2://hadoop102:10000/;principal=hive/hadoop102@EXAMPLE.COM
```

连接后可以发现不用输入帐号密码了



### 全流程调度

为用户创建Kerberos主体

1.在各节点创建hive用户，如已存在则跳过

```
[root@hadoop102 ~]# useradd hive -g hadoop
[root@hadoop102 ~]# echo hive | passwd --stdin hive

[root@hadoop103 ~]# useradd hive -g hadoop
[root@hadoop103 ~]# echo hive | passwd --stdin hive

[root@hadoop104 ~]# useradd hive -g hadoop
[root@hadoop104 ~]# echo hive | passwd --stdin hive
```

2.为hive用户创建Keberos主体

上面创建是为服务端（hive/hadoop102）创建主体，这次是为客户端（hive）创建主体，而这个hive用户是为了脚本使用的

创建主体

```
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey hive"
```

生成keytab文件

```
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/hive.keytab hive"
```

修改keytab文件所有者和访问权限

```
[root@hadoop102 ~]# chown hive:hadoop /etc/security/keytab/hive.keytab
[root@hadoop102 ~]# chmod 440 /etc/security/keytab/hive.keytab
```

分发keytab文件（因为azkaban用的多executor模式）

```
[root@hadoop102 ~]# xsync /etc/security/keytab/hive.keytab
```



**修改数据采集通道**

**用户行为日志**

修改/opt/module/flume/conf/kafka-flume-hdfs.conf配置文件，增加以下参数

```
[root@hadoop104 ~]# vim /opt/module/flume/conf/kafka-flume-hdfs.conf

a1.sinks.k1.hdfs.kerberosPrincipal=hive@EXAMPLE.COM
a1.sinks.k1.hdfs.kerberosKeytab=/etc/security/keytab/hive.keytab
```

**业务数据**

修改sqoop每日同步脚本/home/atguigu/bin/mysql_to_hdfs.sh

```
vim /home/atguigu/bin/mysql_to_hdfs.sh
```

在顶部增加如下认证语句

```
kinit -kt /etc/security/keytab/hive.keytab hive
```



**各数仓脚本修改**

数仓各层脚本均需在顶部加入如下认证语句

```
kinit -kt /etc/security/keytab/hive.keytab hive
```

修改语句如下

```shell
[root@hadoop102 ~]# sed -i '1 a kinit -kt /etc/security/keytab/hive.keytab hive' hdfs_to_ods_log.sh
[root@hadoop102 ~]# sed -i '1 a kinit -kt /etc/security/keytab/hive.keytab hive' hdfs_to_ods_db.sh
[root@hadoop102 ~]# sed -i '1 a kinit -kt /etc/security/keytab/hive.keytab hive' ods_to_dwd_log.sh
[root@hadoop102 ~]# sed -i '1 a kinit -kt /etc/security/keytab/hive.keytab hive' ods_to_dim_db.sh
[root@hadoop102 ~]# sed -i '1 a kinit -kt /etc/security/keytab/hive.keytab hive' ods_to_dwd_db.sh
[root@hadoop102 ~]# sed -i '1 a kinit -kt /etc/security/keytab/hive.keytab hive' dwd_to_dws.sh
[root@hadoop102 ~]# sed -i '1 a kinit -kt /etc/security/keytab/hive.keytab hive' dws_to_dwt.sh
[root@hadoop102 ~]# sed -i '1 a kinit -kt /etc/security/keytab/hive.keytab hive' dwt_to_ads.sh
[root@hadoop102 ~]# sed -i '1 a kinit -kt /etc/security/keytab/hive.keytab hive' hdfs_to_mysql.sh
```

注：

```shell
sed -i '1 a text' file
```

表示将text内容加入到file文件的第1行之后



**修改HDFS特定路径所有者**

1.认证为hdfs用户，执行以下命令并按提示输入密码

```
[root@hadoop102 ~]#  kinit hdfs/hadoop
```

2.修改数据采集目标路径

```
[root@hadoop102 ~]# hadoop fs -chown -R hive:hadoop /origin_data
```

3.修改数仓表所在路径

```
[root@hadoop102 ~]# hadoop fs -chown -R hive:hadoop /warehouse
```

4.修改hive家目录/user/hive

```
[root@hadoop102 ~]# hadoop fs -chown -R hive:hadoop /user/hive
```

5.修改spark.eventLog.dir路径

```
[root@hadoop102 ~]# hadoop fs -chown -R hive:hadoop /spark-history
```



**azkaban调度**

在各节点创建azkaban用户

```
[root@hadoop102 ~]# useradd azkaban -g hadoop
[root@hadoop102 ~]# echo azkaban | passwd --stdin azkaban

[root@hadoop103 ~]# useradd azkaban -g hadoop
[root@hadoop103 ~]# echo azkaban | passwd --stdin azkaban

[root@hadoop104 ~]# useradd azkaban -g hadoop
[root@hadoop104 ~]# echo azkaban | passwd --stdin azkaban
```

2.将各节点Azkaban安装路径所有者改为azkaban用户

```
[root@hadoop102 ~]# chown -R azkaban:hadoop /opt/module/azkaban

[root@hadoop103 ~]# chown -R azkaban:hadoop /opt/module/azkaban

[root@hadoop104 ~]# chown -R azkaban:hadoop /opt/module/azkaban
```

3.使用azkaban用户启动Azkaban

启动Executor Server

在各节点执行以下命令，启动Executor

```
[root@hadoop102 ~]# sudo -i -u azkaban bash -c "cd /opt/module/azkaban/azkaban-exec;bin/start-exec.sh"

[root@hadoop103 ~]# sudo -i -u azkaban bash -c "cd /opt/module/azkaban/azkaban-exec;bin/start-exec.sh"

[root@hadoop104 ~]# sudo -i -u azkaban bash -c "cd /opt/module/azkaban/azkaban-exec;bin/start-exec.sh"
```

激活Executor Server，任选一台节点执行以下激活命令即可

```
[root@hadoop102 ~]# curl http://hadoop102:12321/executor?action=activate

[root@hadoop102 ~]# curl http://hadoop103:12321/executor?action=activate

[root@hadoop102 ~]# curl http://hadoop104:12321/executor?action=activate
```

启动Web Server

```
[root@hadoop102 ~]# sudo -i -u azkaban bash -c "cd /opt/module/azkaban/azkaban-web;bin/start-web.sh"
```

4.修改数仓各层脚本访问权限，确保azkaban用户能够访问到

```
[root@hadoop102 ~]# chown -R atguigu:hadoop /home/atguigu
[root@hadoop102 ~]# chmod 770 /home/atguigu

[root@hadoop103 ~]# chown -R atguigu:hadoop /home/atguigu
[root@hadoop103 ~]# chmod 770 /home/atguigu

[root@hadoop104 ~]# chown -R atguigu:hadoop /home/atguigu
[root@hadoop104 ~]# chmod 770 /home/atguigu
```

之后直接execute azkaban已存在的调度脚本即可

### Kylin配置

**HBase开启认证**

创建用户

```
[root@hadoop102 ~]# useradd -g hadoop hbase
[root@hadoop102 ~]# echo hbase | passwd --stdin hbase

[root@hadoop103 ~]# useradd -g hadoop hbase
[root@hadoop103 ~]# echo hbase | passwd --stdin hbase

[root@hadoop104 ~]# useradd -g hadoop hbase
[root@hadoop104 ~]# echo hbase | passwd --stdin hbase
```

创建hbase Kerberos主体

**注意**，每个节点创建的主体名不一样

在hadoop102节点创建主体，生成密钥文件，并修改所有者

```
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey hbase/hadoop102"
[root@hadoop102 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/hbase.service.keytab hbase/hadoop102"
[root@hadoop102 ~]# chown hbase:hadoop /etc/security/keytab/hbase.service.keytab
```

在hadoop103节点创建主体，生成密钥文件，并修改所有者

```
[root@hadoop103 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey hbase/hadoop103"
[root@hadoop103 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/hbase.service.keytab hbase/hadoop103"
[root@hadoop103 ~]# chown hbase:hadoop /etc/security/keytab/hbase.service.keytab
```

在hadoop104节点创建主体，生成密钥文件，并修改所有者

```
[root@hadoop104 ~]# kadmin -padmin/admin -wadmin -q"addprinc -randkey hbase/hadoop104"
[root@hadoop104 ~]# kadmin -padmin/admin -wadmin -q"xst -k /etc/security/keytab/hbase.service.keytab hbase/hadoop104"
[root@hadoop104 ~]# chown hbase:hadoop /etc/security/keytab/hbase.service.keytab
```

**修改HBase配置文件**

修改$HBASE_HOME/conf/hbase-site.xml配置文件，增加以下参数

```xml
[root@hadoop102 ~]# vim $HBASE_HOME/conf/hbase-site.xml

<property>
  <name>hbase.security.authentication</name>
  <value>kerberos</value>
</property>

<property> 
  <name>hbase.master.kerberos.principal</name> 
  <value>hbase/_HOST@EXAMPLE.COM</value> 
</property> 

<property> 
<name>hbase.master.keytab.file</name> 
<value>/etc/security/keytab/hbase.service.keytab</value> 
</property>

<property>
  <name>hbase.regionserver.kerberos.principal</name> 
  <value>hbase/_HOST@EXAMPLE.COM</value> 
</property> 

<property> 
  <name>hbase.regionserver.keytab.file</name> 
  <value>/etc/security/keytab/hbase.service.keytab</value> 
</property>

<property> 
  <name>hbase.coprocessor.region.classes</name>
  <value>org.apache.hadoop.hbase.security.token.TokenProvider</value>
</property>
```

分发配置文件

```
[root@hadoop102 ~]# xsync $HBASE_HOME/conf/hbase-site.xml
```

**修改hbase.rootdir路径所有者**

使用hdfs/hadoop用户进行认证

```
[root@hadoop102 ~]# kinit hdfs/hadoop
```

修改所有者

```
[root@hadoop102 ~]# hadoop fs -chown -R hbase:hadoop /hbase
```

**启动HBase**

修改各节点HBase安装目录所有者

```
[root@hadoop102 ~]# chown -R hbase:hadoop /opt/module/hbase
[root@hadoop103 ~]# chown -R hbase:hadoop /opt/module/hbase
[root@hadoop104 ~]# chown -R hbase:hadoop /opt/module/hbase
```

配置hbase用户从主节点（hadoop102）到所有节点的ssh免密

使用hbase用户启动HBase

```
[root@hadoop102 ~]# sudo -i -u hbase start-hbase.sh
```

**停止HBase**

启用Kerberos认证之后，关闭HBase时，需先进行Kerberos用户认证，认证的主体为hbase。

1）认证为hbase主体，不然无法关闭hbase

```
[root@hadoop102 ~]# sudo -i -u hbase kinit -kt /etc/security/keytab/hbase.service.keytab hbase/hadoop102
```

2）停止hbase

```
[root@hadoop102 ~]# sudo -i -u hbase stop-hbase.sh
```



**Kylin进行Kerberos认证**

创建kylin系统用户

```
[root@hadoop102 ~]# useradd -g hadoop kylin
[root@hadoop102 ~]# echo kylin | passwd --stdin kylin
```

修改kylin.env.hdfs-working-dir路径所有者为kylin

1）使用hdfs/hadoop用户进行认证

```
[root@hadoop102 ~]# kinit hdfs/hadoop
```

2）修改所有者，这里改成了hive，因为Kylin也是个Hive客户端，以后也会把Kylin认证为Hive，方便取Hive表

```
[root@hadoop102 ~]# hadoop fs -chown -R hive:hadoop /kylin
```

修改/opt/module/kylin所有者为kylin

```
[root@hadoop102 ~]# chown -R kylin:hadoop /opt/module/kylin
```

启动kylin

1）在kylin用户下认证为hive主体

```
[root@hadoop102 ~]# sudo -i -u kylin kinit -kt /etc/security/keytab/hive.keytab hive
```

2）以kylin用户的身份启动kylin

```
[root@hadoop102 ~]# sudo -i -u kylin /opt/module/kylin/bin/kylin.sh start
```



**需要注意**：Kylin构建相同Cube，日期不可以有交集，如果昨天构建15号 - 16 号的Cube，今天就不能再构建15号 - 16 号的Cube了



## Ranger

Apache Ranger是一个Hadoop平台上的全方位数据安全管理框架，它可以为整个Hadoop生态系统提供全面的安全管理。

**Ranger目标**

- 允许用户使用UI或REST API对所有和安全相关的任务进行集中化的管理
- 允许用户使用一个管理工具对操作Hadoop体系中的组件和工具的行为进行细粒度的授权
- 支持Hadoop体系中各个组件的授权认证标准
- 增强了对不同业务场景需求的授权方法支持，例如基于角色的授权或基于属性的授权
- 支持对Hadoop组件所有涉及安全的审计行为的集中化管理

**支持框架**

- Apache Hadoop

- Apache Hive

- Apache HBase

- Apache Storm

- Apache Knox

- Apache Solr

- Apache Kafka

- YARN

- NIFI

**Ranger架构**

![image-20220515142708558](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220515142708558.png)

**工作原理**

Ranager的核心是Web应用程序，也称为RangerAdmin模块，此模块由管理策略，审计日志和报告等三部分组成。

管理员角色的用户可以通过RangerAdmin提供的web界面或REST APIS来定制安全策略。

这些策略会由Ranger提供的轻量级的针对不同Hadoop体系中组件的插件来执行。

插件会在Hadoop的不同组件的核心进程启动后，启动对应的插件进程来进行安全管理

**环境说明**

Ranger2.0要求对应的Hadoop为3.x以上，Hive为3.x以上版本，JDK为1.8以上版本。Hadoop及Hive等需开启用户认证功能，本文基于开启Kerberos安全认证的Hadoop和Hive环境

**创建Kerberos主体**

需要创建四个主体

1. HTTP主体：用于http验证，之前已经创建过，不必再建
2. rangeradmin主体：用于ranger admin组件访问
3. rangerlookup主体：用于访问Hive元数据
4. rangerusersync主体：用于访问用户信息

**使用Hive插件**

Ranger Hive-plugin只能对使用jdbc方式访问hive的请求进行权限管理，hive-cli并不受限制

Ranger 的后续所有操作都是在Web界面上完成，通过对系统用户授权完成权限控制



## Altas

Apache Atlas为组织提供开放式元数据管理和治理功能，用以构建其数据资产目录，对这些资产进行分类和管理，并为数据分析师和数据治理团队，提供围绕这些数据资产的协作功能。

**具体功能**

| 元数据分类 | 支持对元数据进行分类管理，例如个人信息，敏感信息等           |
| ---------- | ------------------------------------------------------------ |
| 元数据检索 | 可按照元数据类型、元数据分类进行检索，支持全文检索           |
| 血缘依赖   | 支持表到表和字段到字段之间的血缘依赖，便于进行问题回溯和影响分析等 |

HBase：作为图数据存储

Solr：元数据检索



**同步Hive信息**

Atlas提供了一个Hive元数据导入的脚本，直接执行该脚本，即可完成Hive元数据的初次全量导入。

```shell
[root@hadoop102 ~]# /opt/module/atlas/hook-bin/import-hive.sh 
```

输入密码 admin

```
Enter username for atlas :- admin
Enter password for atlas :- admin
```

等待片刻，出现以下日志，即表明导入成功

```
Hive Meta Data import was successful!!!
```

Atlas给Hive配置hive hook之后，hive提交的sql语句都会通过kafka传递到Atlas。之后atlas解析这些sql语句去完成血缘关系的搭建

可以通过Web页面感受Atlas的使用方式



## 各框架启停脚本

root用户执行

**启动**

```shell
hdp.sh start
sudo -i -u mapred mapred --daemon start historyserver
zk.sh  start
kf.sh  start
hiveserver2                                   # hiveserver2
/opt/module/hive/bin/hive --service metastore # hive metastore
sudo -i -u hbase start-hbase.sh
sudo -i -u ranger ranger-admin start     # ranger-admin模块 
sudo -i -u ranger ranger-usersync start  # ranger-usersync模块
sudo -i -u solr /opt/module/solr/bin/solr start # solr，三个节点都要启动
/opt/module/atlas/bin/atlas_start.py     # atlas
```

**停止**

```shell
sudo -i -u ranger ranger-admin stop
sudo -i -u ranger ranger-usersync stop
/opt/module/atlas/bin/atlas_stop.py
sudo -i -u solr /opt/module/solr/bin/solr stop # 三个节点都要运行
sudo -i -u hbase stop-hbase.sh
kf.sh  stop
zk.sh  stop
sudo -i -u mapred mapred --daemon stop historyserver
hdp.sh stop
```

## 数据质量管理

**数据质量评价标准**

| 评价标准 | 描述                                         | 监控项                                       |
| ----------------------------- | -------------------------------------------- | -------------------------------------------- |
| 唯一性   | 指主键保持唯一                               | 字段唯一性检查                               |
| 完整性   | 主要包括记录缺失和字段值缺失等方面           | 字段枚举值检查、字段记录数检查、字段空值检查 |
| 精确度   | 数据生成的正确性，数据在整个链路流转的正确性 | 波动阀值检查                                 |
| 合法性   | 主要包括格式、类型、域值的合法性             | 字段日期格式检查、字段长度检查、字段值域检查 |
| 时效性   | 主要包括数据处理的时效性                     | 批处理是否按时完成                           |

**监控指标举例**

ODS层数据量，每日环比和每周同比变化不能超过一定范围

DIM层不能出现id空值，重复值；

DWD层不能出现id空值，重复值；

在每层中任意挑选一张表作为示例。

![image-20220515212612454](https://raw.githubusercontent.com/flickever/NotePictures/master/%E9%A1%B9%E7%9B%AE/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93image-20220515212612454.png)

**建表语句**

空值指标表，null_id

```sql
CREATE TABLE data_supervisor.`null_id`
(
    `dt`                 date        NOT NULL COMMENT '日期',
    `tbl`                varchar(50) NOT NULL COMMENT '表名',
    `col`                varchar(50) NOT NULL COMMENT '列名',
    `value`              int         DEFAULT NULL COMMENT '空ID个数',
    `value_min`          int         DEFAULT NULL COMMENT '下限',
    `value_max`          int         DEFAULT NULL COMMENT '上限',
    `notification_level` int         DEFAULT NULL COMMENT '警告级别',
    PRIMARY KEY (`dt`, `tbl`, `col`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8
    comment '空值指标表';
```

重复值指标表，duplicate

```sql
CREATE TABLE data_supervisor.`duplicate`
(
    `dt`                 date        NOT NULL COMMENT '日期',
    `tbl`                varchar(50) NOT NULL COMMENT '表名',
    `col`                varchar(50) NOT NULL COMMENT '列名',
    `value`              int         DEFAULT NULL COMMENT '重复值个数',
    `value_min`          int         DEFAULT NULL COMMENT '下限',
    `value_max`          int         DEFAULT NULL COMMENT '上限',
    `notification_level` int         DEFAULT NULL COMMENT '警告级别',
    PRIMARY KEY (`dt`, `tbl`, `col`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8
    comment '重复值指标表';
```

值域指标表，rng

```sql
CREATE TABLE data_supervisor.`rng`
(
    `dt`                 date        NOT NULL COMMENT '日期',
    `tbl`                varchar(50) NOT NULL COMMENT '表名',
    `col`                varchar(50) NOT NULL COMMENT '列名',
    `value`              int         DEFAULT NULL COMMENT '超出预定值域个数',
    `range_min`          int         DEFAULT NULL COMMENT '值域下限',
    `range_max`          int         DEFAULT NULL COMMENT '值域上限',
    `value_min`          int         DEFAULT NULL COMMENT '下限',
    `value_max`          int         DEFAULT NULL COMMENT '上限',
    `notification_level` int         DEFAULT NULL COMMENT '警告级别',
    PRIMARY KEY (`dt`, `tbl`, `col`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8
    comment '值域指标表';
```

环比增长指标表，day_on_day

```sql
CREATE TABLE data_supervisor.`day_on_day`
(
    `dt`                 date        NOT NULL COMMENT '日期',
    `tbl`                varchar(50) NOT NULL COMMENT '表名',
    `value`              double DEFAULT NULL COMMENT '环比增长百分比',
    `value_min`          double DEFAULT NULL COMMENT '增长上限',
    `value_max`          double DEFAULT NULL COMMENT '增长上限',
    `notification_level` int    DEFAULT NULL COMMENT '警告级别',
    PRIMARY KEY (`dt`, `tbl`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8
    comment '环比增长指标表';
```

同比增长指标表，week_on_week

```sql
CREATE TABLE data_supervisor.`week_on_week`
(
    `dt`                 date        NOT NULL COMMENT '日期',
    `tbl`                varchar(50) NOT NULL COMMENT '表名',
    `value`              double DEFAULT NULL COMMENT '同比增长百分比',
    `value_min`          double DEFAULT NULL COMMENT '增长上限',
    `value_max`          double DEFAULT NULL COMMENT '增长上限',
    `notification_level` int    DEFAULT NULL COMMENT '警告级别',
    PRIMARY KEY (`dt`, `tbl`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8
    comment '同比增长指标表';
```

### 检测脚本

**空id检查**

计算空值个数，并将结果和自己定义的阈值上下限，插入到MySQL表中

```shell
#!/usr/bin/env bash
# -*- coding: utf-8 -*-
# 检查id空值
# 解析参数
while getopts "t:d:c:s:x:l:" arg; do
  case $arg in
  # 要处理的表名
  t)
    TABLE=$OPTARG
    ;;
  # 日期
  d)
    DT=$OPTARG
    ;;
  # 要计算空值的列名
  c)
    COL=$OPTARG
    ;;
  # 空值指标下限
  s)
    MIN=$OPTARG
    ;;
  # 空值指标上限
  x)
    MAX=$OPTARG
    ;;
  # 告警级别
  l)
    LEVEL=$OPTARG
    ;;
  ?)
    echo "unkonw argument"
    exit 1
    ;;
  esac
done

#如果dt和level没有设置，那么默认值dt是昨天 告警级别是0
[ "$DT" ] || DT=$(date -d '-1 day' +%F)
[ "$LEVEL" ] || LEVEL=0

# 数仓DB名称
HIVE_DB=gmall

# 查询引擎
HIVE_ENGINE=hive

# MySQL相关配置
mysql_user="root"
mysql_passwd="000000"
mysql_host="hadoop102"
mysql_DB="data_supervisor"
mysql_tbl="null_id"

# 认证为hive用户，如在非安全(Hadoop未启用Kerberos认证)环境中，则无需认证
kinit -kt /etc/security/keytab/hive.keytab hive

# 空值个数
RESULT=$($HIVE_ENGINE -e "set hive.cli.print.header=false;select count(1) from $HIVE_DB.$TABLE where dt='$DT' and $COL is null;")

#结果插入MySQL
mysql -h"$mysql_host" -u"$mysql_user" -p"$mysql_passwd" \
  -e"INSERT INTO $mysql_DB.$mysql_tbl VALUES('$DT', '$TABLE', '$COL', $RESULT, $MIN, $MAX, $LEVEL)
ON DUPLICATE KEY UPDATE \`value\`=$RESULT, value_min=$MIN, value_max=$MAX, notification_level=$LEVEL;"
```

**重复id检查**

计算重复值个数，并将结果和自己定义的阈值上下限，插入到MySQL表中

```shell
#!/usr/bin/env bash
# -*- coding: utf-8 -*-
# 监控某张表一列的重复值
# 参数解析
while getopts "t:d:c:s:x:l:" arg; do
  case $arg in
  # 要处理的表名
  t)
    TABLE=$OPTARG
    ;;
  # 日期
  d)
    DT=$OPTARG
    ;;
  # 要计算重复值的列名
  c)
    COL=$OPTARG
    ;;
  # 重复值指标下限
  s)
    MIN=$OPTARG
    ;;
  # 重复值指标上限
  x)
    MAX=$OPTARG
    ;;
  # 告警级别
  l)
    LEVEL=$OPTARG
    ;;
  ?)
    echo "unkonw argument"
    exit 1
    ;;
  esac
done

#如果dt和level没有设置，那么默认值dt是昨天 告警级别是0
[ "$DT" ] || DT=$(date -d '-1 day' +%F)
[ "$LEVEL" ] || LEVEL=0

# 数仓DB名称
HIVE_DB=gmall

# 查询引擎
HIVE_ENGINE=hive

# MySQL相关配置
mysql_user="root"
mysql_passwd="000000"
mysql_host="hadoop102"
mysql_DB="data_supervisor"
mysql_tbl="duplicate"

# 认证为hive用户，如在非安全(Hadoop未启用Kerberos认证)环境中，则无需认证
kinit -kt /etc/security/keytab/hive.keytab hive

# 重复值个数
RESULT=$($HIVE_ENGINE -e "set hive.cli.print.header=false;select count(1) from (select $COL from $HIVE_DB.$TABLE where dt='$DT' group by $COL having count($COL)>1) t1;")

# 将结果插入MySQL
mysql -h"$mysql_host" -u"$mysql_user" -p"$mysql_passwd" \
  -e"INSERT INTO $mysql_DB.$mysql_tbl VALUES('$DT', '$TABLE', '$COL', $RESULT, $MIN, $MAX, $LEVEL)
ON DUPLICATE KEY UPDATE \`value\`=$RESULT, value_min=$MIN, value_max=$MAX, notification_level=$LEVEL;"
```

**值域检查**

计算超出规定值域的值的个数，并将结果和自己定义的阈值上下限，插入到MySQL表中

```shell
#!/usr/bin/env bash
# -*- coding: utf-8 -*-
# 计算某一列异常值个数

while getopts "t:d:l:c:s:x:a:b:" arg; do
  case $arg in
  # 要处理的表名
  t)
    TABLE=$OPTARG
    ;;
  # 日替
  d)
    DT=$OPTARG
    ;;
  # 要处理的列
  c)
    COL=$OPTARG
    ;;
  # 不在规定值域的值的个数下限
  s)
    MIN=$OPTARG
    ;;
  # 不在规定值域的值的个数上限
  x)
    MAX=$OPTARG
    ;;
  # 告警级别
  l)
    LEVEL=$OPTARG
    ;;
  # 规定值域为a-b
  a)
    RANGE_MIN=$OPTARG
    ;;
  b)
    RANGE_MAX=$OPTARG
    ;;
  ?)
    echo "unkonw argument"
    exit 1
    ;;
  esac
done

#如果dt和level没有设置，那么默认值dt是昨天 告警级别是0
[ "$DT" ] || DT=$(date -d '-1 day' +%F)
[ "$LEVEL" ] || LEVEL=0

# 数仓DB名称
HIVE_DB=gmall

# 查询引擎
HIVE_ENGINE=hive

# MySQL相关配置
mysql_user="root"
mysql_passwd="000000"
mysql_host="hadoop102"
mysql_DB="data_supervisor"
mysql_tbl="rng"

# 认证为hive用户，如在非安全(Hadoop未启用Kerberos认证)环境中，则无需认证
kinit -kt /etc/security/keytab/hive.keytab hive

# 查询不在规定值域的值的个数
RESULT=$($HIVE_ENGINE -e "set hive.cli.print.header=false;select count(1) from $HIVE_DB.$TABLE where dt='$DT' and $COL not between $RANGE_MIN and $RANGE_MAX;")

# 将结果写入MySQL
mysql -h"$mysql_host" -u"$mysql_user" -p"$mysql_passwd" \
  -e"INSERT INTO $mysql_DB.$mysql_tbl VALUES('$DT', '$TABLE', '$COL', $RESULT, $RANGE_MIN, $RANGE_MAX, $MIN, $MAX, $LEVEL)
ON DUPLICATE KEY UPDATE \`value\`=$RESULT, range_min=$RANGE_MIN, range_max=$RANGE_MAX, value_min=$MIN, value_max=$MAX, notification_level=$LEVEL;"
```

**数据量环比检查脚本**

计算数据量环比增长值，并将结果和自己定义的阈值上下限，插入到MySQL表中

```shell
#!/usr/bin/env bash
# -*- coding: utf-8 -*-
# 计算一张表单日数据量环比增长值
# 参数解析
while getopts "t:d:s:x:l:" arg; do
  case $arg in
  # 要处理的表名
  t)
    TABLE=$OPTARG
    ;;
  # 日期
  d)
    DT=$OPTARG
    ;;
  # 环比增长指标下限
  s)
    MIN=$OPTARG
    ;;
  # 环比增长指标上限
  x)
    MAX=$OPTARG
    ;;
  # 告警级别
  l)
    LEVEL=$OPTARG
    ;;
  ?)
    echo "unkonw argument"
    exit 1
    ;;
  esac
done

#如果dt和level没有设置，那么默认值dt是昨天 告警级别是0
[ "$DT" ] || DT=$(date -d '-1 day' +%F)
[ "$LEVEL" ] || LEVEL=0

# 数仓DB名称
HIVE_DB=gmall

# 查询引擎
HIVE_ENGINE=hive

# MySQL相关配置
mysql_user="root"
mysql_passwd="000000"
mysql_host="hadoop102"
mysql_DB="data_supervisor"
mysql_tbl="day_on_day"

# 认证为hive用户，如在非安全(Hadoop未启用Kerberos认证)环境中，则无需认证
kinit -kt /etc/security/keytab/hive.keytab hive

# 昨日数据量
YESTERDAY=$($HIVE_ENGINE -e "set hive.cli.print.header=false; select count(1) from $HIVE_DB.$TABLE where dt=date_add('$DT',-1);")

# 今日数据量
TODAY=$($HIVE_ENGINE -e "set hive.cli.print.header=false;select count(1) from $HIVE_DB.$TABLE where dt='$DT';")

# 计算环比增长值
if [ "$YESTERDAY" -ne 0 ]; then
  RESULT=$(awk "BEGIN{print ($TODAY-$YESTERDAY)/$YESTERDAY*100}")
else
  RESULT=10000
fi

# 将结果写入MySQL表格
mysql -h"$mysql_host" -u"$mysql_user" -p"$mysql_passwd" \
  -e"INSERT INTO $mysql_DB.$mysql_tbl VALUES('$DT', '$TABLE', $RESULT, $MIN, $MAX, $LEVEL)
ON DUPLICATE KEY UPDATE \`value\`=$RESULT, value_min=$MIN, value_max=$MAX, notification_level=$LEVEL;"
```

**数据量同比检查**

计算数据量同比增长值，并将结果和自己定义的阈值上下限，插入到MySQL表中

```shell
#!/usr/bin/env bash
# -*- coding: utf-8 -*-
# 计算一张表一周数据量同比增长值
# 参数解析
while getopts "t:d:s:x:l:" arg; do
  case $arg in
  # 要处理的表名
  t)
    TABLE=$OPTARG
    ;;
  # 日期
  d)
    DT=$OPTARG
    ;;
  # 同比增长指标下限
  s)
    MIN=$OPTARG
    ;;
  # 同比增长指标上限
  x)
    MAX=$OPTARG
    ;;
  # 告警级别
  l)
    LEVEL=$OPTARG
    ;;
  ?)
    echo "unkonw argument"
    exit 1
    ;;
  esac
done

#如果dt和level没有设置，那么默认值dt是昨天 告警级别是0
[ "$DT" ] || DT=$(date -d '-1 day' +%F)
[ "$LEVEL" ] || LEVEL=0

# 数仓DB名称
HIVE_DB=gmall

# 查询引擎
HIVE_ENGINE=hive

# MySQL相关配置
mysql_user="root"
mysql_passwd="000000"
mysql_host="hadoop102"
mysql_DB="data_supervisor"
mysql_tbl="week_on_week"

# 认证为hive用户，如在非安全(Hadoop未启用Kerberos认证)环境中，则无需认证
kinit -kt /etc/security/keytab/hive.keytab hive

# 上周数据量
LASTWEEK=$($HIVE_ENGINE -e "set hive.cli.print.header=false;select count(1) from $HIVE_DB.$TABLE where dt=date_add('$DT',-7);")

# 本周数据量
THISWEEK=$($HIVE_ENGINE -e "set hive.cli.print.header=false;select count(1) from $HIVE_DB.$TABLE where dt='$DT';")

# 计算增长
if [ $LASTWEEK -ne 0 ]; then
  RESULT=$(awk "BEGIN{print ($THISWEEK-$LASTWEEK)/$LASTWEEK*100}")
else
  RESULT=10000
fi

# 将结果写入MySQL
mysql -h"$mysql_host" -u"$mysql_user" -p"$mysql_passwd" \
  -e"INSERT INTO $mysql_DB.$mysql_tbl VALUES('$DT', '$TABLE', $RESULT, $MIN, $MAX, $LEVEL)
ON DUPLICATE KEY UPDATE \`value\`=$RESULT, value_min=$MIN, value_max=$MAX, notification_level=$LEVEL;"
```



### 数仓各层检测

**ODS层**

```shell
#!/usr/bin/env bash
DT=$1
[ "$DT" ] || DT=$(date -d '-1 day' +%F)

#检查表 ods_order_info 数据量日环比增长
#参数： -t 表名
#      -d 日期
#      -s 环比增长下限
#      -x 环比增长上限
#      -l 告警级别
bash day_on_day.sh -t ods_order_info -d "$DT" -s -10 -x 10 -l 1

#检查表 ods_order_info 数据量周同比增长
#参数： -t 表名
#      -d 日期
#      -s 同比增长下限
#      -x 同比增长上限
#      -l 告警级别
bash week_on_week.sh -t ods_order_info -d "$DT" -s -10 -x 50 -l 1

#检查表 ods_order_info 订单异常值
#参数： -t 表名
#      -d 日期
#      -s 指标下限
#      -x 指标上限
#      -l 告警级别
#      -a 值域下限
#      -b 值域上限
bash range.sh -t ods_order_info -d "$DT" -c final_amount -a 0 -b 100000 -s 0 -x 100 -l 1 
```



**DWD层**

```shell
#!/usr/bin/env bash
DT=$1
[ "$DT" ] || DT=$(date -d '-1 day' +%F)

# 检查表 dwd_order_info 重复ID
#参数： -t 表名
#      -d 日期
#      -c 检查重复值的列
#      -s 异常指标下限
#      -x 异常指标上限
#      -l 告警级别
bash duplicate.sh -t dwd_order_info -d "$DT" -c id -s 0 -x 5 -l 0

#检查表 dwd_order_info 的空ID
#参数： -t 表名
#      -d 日期
#      -c 检查空值的列
#      -s 异常指标下限
#      -x 异常指标上限
#      -l 告警级别
bash null_id.sh -t dwd_order_info -d "$DT" -c id -s 0 -x 10 -l 0
```



**DIM层**

```shell
#!/usr/bin/env bash
DT=$1
[ "$DT" ] || DT=$(date -d '-1 day' +%F)

#检查表 dim_user_info 的重复ID
#参数： -t 表名
#      -d 日期
#      -c 检查重复值的列
#      -s 异常指标下限
#      -x 异常指标上限
#      -l 告警级别
bash duplicate.sh -t dim_user_info -d "$DT" -c id -s 0 -x 5 -l 0

#检查表 dim_user_info 的空ID
#参数： -t 表名
#      -d 日期
#      -c 检查空值的列
#      -s 异常指标下限
#      -x 异常指标上限
#      -l 告警级别
bash null_id.sh -t dim_user_info -d "$DT" -c id -s 0 -x 10 -l 0
```



### 告警模块

集成睿象云

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import mysql.connector
import sys
import smtplib
from email.mime.text import MIMEText
from email.header import Header
import datetime
import urllib
import urllib2
import random


def get_yesterday():
    """
    :return: 前一天的日期
    """
    today = datetime.date.today()
    one_day = datetime.timedelta(days=1)
    yesterday = today - one_day
    return str(yesterday)


def read_table(table, dt):
    """
    :param table:读取的表名
    :param dt:读取的数据日期
    :return:表中的异常数据(统计结果超出规定上下限的数据)
    """

    # mysql必要参数设置，需根据实际情况作出修改
    mysql_user = "root"
    mysql_password = "000000"
    mysql_host = "hadoop102"
    mysql_schema = "data_supervisor"

    # 获取Mysql数据库连接
    connect = mysql.connector.connect(user=mysql_user, password=mysql_password, host=mysql_host, database=mysql_schema)
    cursor = connect.cursor()

    # 查询表头
    # ['dt', 'tbl', 'col', 'value', 'value_min', 'value_max', 'notification_level']
    query = "desc " + table
    cursor.execute(query)
    head = map(lambda x: str(x[0]), cursor.fetchall())

    # 查询异常数据(统计结果超出规定上下限的数据)
    # [(datetime.date(2021, 7, 16), u'dim_user_info', u'id', 7, 0, 5, 1),
    # (datetime.date(2021, 7, 16), u'dwd_order_id', u'id', 10, 0, 5, 1)]
    query = ("select * from " + table + " where dt='" + dt + "' and `value` not between value_min and value_max")
    cursor.execute(query)
    cursor_fetchall = cursor.fetchall()

    # 将指标和表头映射成为dict数组
    #[{'notification_level': 1, 'value_min': 0, 'value': 7, 'col': u'id', 'tbl': u'dim_user_info', 'dt': datetime.date(2021, 7, 16), 'value_max': 5},
    # {'notification_level': 1, 'value_min': 0, 'value': 10, 'col': u'id', 'tbl': u'dwd_order_id', 'dt': datetime.date(2021, 7, 16), 'value_max': 5}]
    fetchall = map(lambda x: dict(x), map(lambda x: zip(head, x), cursor_fetchall))
    return fetchall


def one_alert(line):
    """
    集成第三方告警平台睿象云，使用其提供的通知媒介发送告警信息
    :param line: 一个等待通知的异常记录，{'notification_level': 1, 'value_min': 0, 'value': 7, 'col': u'id', 'tbl': u'dim_user_info', 'dt': datetime.date(2021, 7, 16), 'value_max': 5}
    """

    # 集成睿象云需要使用的rest接口，和APP KEY，须在睿象云平台获取
    one_alert_key = "c2030c9a-7896-426f-bd64-59a8889ac8e3"
    one_alert_host = "http://api.aiops.com/alert/api/event"

    # 根据睿象云的rest api要求，传入必要的参数
    data = {
        "app": one_alert_key,
        "eventType": "trigger",
        "eventId": str(random.randint(10000, 99999)),
        "alarmName": "".join(["表格", str(line["tbl"]), "数据异常."]),
        "alarmContent": "".join(["指标", str(line["norm"]), "值为", str(line["value"]),
                                 ", 应为", str(line["value_min"]), "-", str(line["value_max"]),
                                 ", 参考信息：" + str(line["col"]) if line.get("col") else ""]),
        "priority": line["notification_level"] + 1
    }

    # 使用urllib和urllib2向睿象云的rest结构发送请求，从而触发睿象云的通知策略
    body = urllib.urlencode(data)
    request = urllib2.Request(one_alert_host, body)
    urlopen = urllib2.urlopen(request).read().decode('utf-8')
    print urlopen


def mail_alert(line):
    """
    使用电子邮件的方式发送告警信息
    :param line: 一个等待通知的异常记录，{'notification_level': 1, 'value_min': 0, 'value': 7, 'col': u'id', 'tbl': u'dim_user_info', 'dt': datetime.date(2021, 7, 16), 'value_max': 5}
    """

    # smtp协议发送邮件的必要设置
    mail_host = "smtp.126.com"
    mail_user = "skiinder@126.com"
    mail_pass = "KADEMQZWCPFWZETF"

    # 告警内容
    message = ["".join(["表格", str(line["tbl"]), "数据异常."]),
               "".join(["指标", str(line["norm"]), "值为", str(line["value"]),
                        ", 应为", str(line["value_min"]), "-", str(line["value_max"]),
                        ", 参考信息：" + str(line["col"]) if line.get("col") else ""])]
    # 告警邮件，发件人
    sender = mail_user

    # 告警邮件，收件人
    receivers = [mail_user]

    # 将邮件内容转为html格式
    mail_content = MIMEText("".join(["<html>", "<br>".join(message), "</html>"]), "html", "utf-8")
    mail_content["from"] = sender
    mail_content["to"] = receivers[0]
    mail_content["Subject"] = Header(message[0], "utf-8")

    # 使用smtplib发送邮件
    try:
        smtp = smtplib.SMTP_SSL()
        smtp.connect(mail_host, 465)
        smtp.login(mail_user, mail_pass)
        content_as_string = mail_content.as_string()
        smtp.sendmail(sender, receivers, content_as_string)
    except smtplib.SMTPException as e:
        print e


def main(argv):
    """
    :param argv: 系统参数，共三个，第一个为python脚本本身，第二个为告警方式，第三个为日期
    """
    
    # 如果没有传入日期参数，将日期定为昨天
    if len(argv) >= 3:
        dt = argv[2]
    else:
        dt = get_yesterday()

    notification_level = 0

    # 通过参数设置告警方式，默认是睿象云
    alert = None
    if len(argv) >= 2:
        alert = {
            "mail": mail_alert,
            "one": one_alert
        }[argv[1]]
    if not alert:
        alert = one_alert

    # 遍历所有表，查询所有错误内容，如果大于设定警告等级，就发送警告
    for table in ["day_on_day", "duplicate", "null_id", "rng", "week_on_week"]:
        for line in read_table(table, dt):
            if line["notification_level"] >= notification_level:
                line["norm"] = table
                alert(line)


if __name__ == "__main__":
    # 两个命令行参数
    # 第一个为警告类型：one或者mail
    # 第二个为日期，留空取昨天
    main(sys.argv)
```



### Azkaban调度

由于数据质量监控依赖于数据仓库工作流，直接配置依赖又会导致过耦合

使用Azkaban API主动监视数据仓库工作流的执行状态，进而触发数据质量监控工作流

该脚本主要是对Azkaban API的封装，主要有三个方法：

-  login函数可以登录Azkanban并返回session_id
- get_exec_id函数可以获取正在执行的工作流程的Execution ID
- wait_node可以等待指定Flow中某一结点执行完毕并判断其是否执行成功

**脚本代码**

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import time
import urllib
import urllib2
import json

# Azkaban API 接口地址
az_url = "http://hadoop102:8081/"
# Azkaban用户名
az_username = "atguigu"
# Azkaban密码
az_password = "atguigu"
# 工程名称
project = "gmall"
# flow名称
flow = "gmall"


def post(url, data):
    """
    发送post请求到指定网址

    :param url: 指定网址
    :param data: 请求参数
    :return: 请求结果
    """
    body = urllib.urlencode(data)
    request = urllib2.Request(url, body)
    urlopen = urllib2.urlopen(request).read().decode('utf-8')
    return json.loads(urlopen)


def get(url, data):
    """
    发送get请求到指定网址

    :param url: 指定网址
    :param data: 请求参数
    :return: 请求结果
    """
    body = urllib.urlencode(data)
    urlopen = urllib2.urlopen(url + body).read().decode('utf-8')
    return json.loads(urlopen)


def login():
    """
    使用`Authenticate`API进行azkaban身份认证，获取session ID

    :return: 返回session_id
    """
    data = {
        "action": "login",
        "username": az_username,
        "password": az_password
    }
    auth = post(az_url, data)
    return str(auth.get(u"session.id"))


def get_exec_id(session_id):
    """
    使用`Fetch Running Executions of a Flow`API获取正在执行的Flow的ExecId

    :param session_id: 和azkaban通讯的session_id
    :param project: 项目名称
    :param flow: 工作流名称
    :return: 执行ID
    """
    data = {
        "session.id": session_id,
        "ajax": "getRunning",
        "project": project,
        "flow": flow
    }
    execs = get(az_url + "executor?", data).get(u"execIds")
    if execs:
        return str(execs[0])
    else:
        return None


def wait_node(session_id, exec_id, node_id):
    """
    循环使用`Fetch a Flow Execution`API获取指定Flow中的某个节点(job)的执行状态，直到其执行完成

    :param session_id: 和azkaban通讯的session_id
    :param exec_id: 执行ID
    :param node_id: 指定节点(job)
    :return: 该节点是否成功执行完毕
    """
    data = {
        "session.id": session_id,
        "ajax": "fetchexecflow",
        "execid": exec_id
    }
    status = None

    # 若指定Flow中的指定Node(job)的执行状态是未完成的状态，就一直循环
    while status not in ["SUCCEEDED", "FAILED", "CANCELLED", "SKIPPED", "KILLED"]:
        # 获取指定Flow的当前的执行信息
        flow_exec = get(az_url + "executor?", data)
        # 从该Flow的执行信息中获取nodes字段的值，并遍历寻找特定的节点(job)信息，进而获取该节点(job)的状态
        for node in flow_exec.get(u"nodes"):
            if unicode(node_id) == node.get(u"id"):
                status = str(node.get(u"status"))
        print " ".join([node_id, status])
        # 等待1s，进入下一轮循环判断
        time.sleep(1)
    return status == "SUCCEEDED"
```

**ODS检验脚本**

调用上面的脚本来检测数据质量

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import sys
import os
from azclient import login,wait_node,get_exec_id
from check_notification import get_yesterday

def check_ods(dt, session_id, exec_id):
    """
    检查ODS层数据质量

    :param dt: 日期
    :param session_id: 和azkaban通讯的session_id
    :param exec_id: 指定的执行ID
    :return: None
    """
    if wait_node(session_id, exec_id, "hdfs_to_ods_db") and wait_node(session_id, exec_id, "hdfs_to_ods_log"):
        os.system("bash check_ods.sh " + dt)

if __name__ == '__main__':
    argv = sys.argv
    # 获取session_id
    session_id = login()

    # 获取执行ID。只有在原Flow正在执行时才能获取
    exec_id = get_exec_id(session_id)

    # 获取日期，如果不存在取昨天
    if len(argv) >= 2:
        dt = argv[1]
    else:
        dt = get_yesterday()

    # 检查各层数据质量
    if exec_id:
        check_ods(dt, session_id, exec_id)
```

