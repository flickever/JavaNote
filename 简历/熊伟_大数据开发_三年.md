# 熊伟\_大数据开发_三年

## 个人信息

- 姓   名：熊伟                            	  						年龄：26
- 工作经验：3年                                                       学历：本科
- 目前状况：在职                                                     岗位：IT开发工程师
- 电话：18339955330                                             邮箱：xiongv.ever@outlook.com

## 专业技能

- .掌握Java和Scala编程；
- 掌握Hadoop架构，熟悉HDFS的基本操作与原理，熟悉Yarn的任务分配调度流程；
- 掌握HiveSQL的编写，及数据分层、建模、优化等；
- 掌握Spark的RDD 原理，SparkStreaming的编程模型以及SparkSQL的编写使用；
- 掌握Kafka相关原理，使用Kafka构建实时数据流管道，与Flink实时数据流整合；
- 掌握Zookeeper的运行机制，能够搭建zk集群，了解基于Zookeeper的特性的典型应用；
- 掌握Hbase的存储机制以及优化；
- 掌握Flume日志数据收集框架，根据业务需求对配置文件进行相应的修改，满足业务需求；
- 掌握使用Sqoop/DataX对数据和数据库之间进行出导入导出；
- 了解使用Python开发；
- 了解Flink的水位线和背压机制，使用过Flink CEP进行代码开发；
- 了解Druid，Kylin等OLAP分析引擎；

## 工作经历

**在职时间**                      **所属行业**         **公司名称**                                                       **担任职位**

2017.06--2018.04       电子商务	    郑州万国优品保税进出口有限公司             数据开发工程师

2018.07--2020.02       智能硬件        宁波舜宇光学科技有限公司                         数据开发工程师

2020.05--至今             手机通讯        华勤技术股份有限公司                                 数据平台开发

## 项目经历

**2020.8-至今**                                                              **项目名称：华勤质量云中枢离线项目**

**项目描述：**

​		为对公司生产、售后过程中产品质量问题进行监控、可视化展示供管理层决策并对制造部门考核而发起。从业务系统中抽取生产、检验、售后信息上云，以ERP主数据作为维度信息实现分析，分析结果传入关系型数据库供帆软调用；

​		部分指标业务方要自选时间分析数据，所以要在PG中建立存储过程，对ADS层最细粒度的数据再次聚合，得到指标在看板上展示；

**软件架构：DataWorks + E-MapReduce + OSS + ADB PostgreSQL + Serverless +  FineReport** 

**责任描述：**

​	**模型开发：**根据业务方需求，抽取业务系统数据上云，确定业务逻辑，实现从ODS、CDM到ADS以及ADB PG一整套模型开发；

​	**数据分析**：创建生产、检验、售后信息事实表，根据提供指标进行相关分析，满足业务需求；

​	**平台运维**：负责阿里云平台的运维保障，配置调整，成功解决过一次集群内存泄漏问题，并给出预警办法；

​	**任务调优：**对项目重点任务调优，帮助解决部分Hive任务频繁OOM、以及运行时间过长问题；

​	**平台开发：**阿里云平台二次开发，对Hive任务产生的Yarn任务进行分析，如分析任务时长、或执行频率等；



**2019.5-2020.2**                                                       **项目名称：舜宇工业大数据项目[实时]**

**项目描述：**

​		舜宇工业大数据项目是舜宇光电为提升产品良率，采集工业生产各个环节实时信息，实时分析处理后发送反馈信息，通过收集原料、机台、操作日志信息等，分析处理后及时得到实时生产信息，帮助生产部门领导及现场技术人员得到各环节良率信息，便于在良率降低时及时响应，快速定位问题，降低企业生产成本； 

**软件架构：Mysql + Flink + Kafka + Canal + Redis + Hbase + Phoenix + Driuid + Flume**

**责任描述：**

​	**数据同步**：产品状态信息存储于MySQL，一旦发生改变，Cancal获取bin-log更改信息下沉到Kafka的ods层；存于MySQL中的维度信息如果发生更改，由Canal 发送到Flink程序，Flink程序处理后同步修改Redis中的维度数据；

​	**数据处理**：主要利用Flink程序消费Kafka中ods层字段，对数据进行过滤拉宽，处理完成的数据向Kafka的dw层和HBase各发送一份；

​	**数据分析**：Druid预处理Kafka中dw层数据，保存计算后数据；

​	               	Phoenix查询Hbase数据，构建Hbase二级索引，提升查询效率；



**2018.7-2019.4**                                                    **项目名称：舜宇工业大数据项目[离线]**

**项目描述：**

为分析深入分析良率影响因素，搭建离线数仓分析数据，离线分析有助于部门领导及现场技术人员通过查看某一周期的生产数据，以直观的统计数据查找到对于生产影响较大的信息，如机台Mac、供应商来料、不良集中工序等；

**软件架构：MySQL+Hadoop+Hive+SparkSQL +Kylin+HBase+DataX**

**责任描述：**

​	**数据采集：**    使用DataX同步MySQL数据到Hive的ods层，供后续数据处理； Flume获取生产日志信息，同步到HDFS，再使用SparkSQL将HDFS数据同步到hive的ods层；

​	**数据处理：**    使用SparkSQL进行数据ETL操作； Spark范围分区解决数据倾斜问题；

​	**数据分析：**    Kylin预计算作为数据仓库的ads层；



## **自我评价**

1. 有较强的适应能力和接触新事物的能力，学习能力较强；
2. 能很快适应新的工作环境,融入团队中去，善于学习新的知识,有一定的自学能力；
3. 乐于与用户以及同事和领导沟通，以便快速解决项目遇到的问题；
4. 喜欢互联网行业，对新技术有较强的学习能力；

